{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çŠ¶æ€ç : 200\n",
      "å†…å®¹å¤§å°: 27053 å­—ç¬¦\n",
      "\n",
      "åŒ…å«'å›ç­”äº'çš„è¡Œ:\n",
      "\n",
      "è¯·å°† debug_output.txt æ–‡ä»¶çš„å‰1000å­—ç¬¦å‘ç»™æˆ‘\n"
     ]
    }
   ],
   "source": [
    "#è§£æé¡µé¢ç»“æ„\n",
    "import requests\n",
    "\n",
    "url = \"https://answer.chancefoundation.org.cn/\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "# ä¿å­˜åŸå§‹å“åº”\n",
    "with open('debug_output.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(f\"çŠ¶æ€ç : {response.status_code}\")\n",
    "print(f\"å†…å®¹å¤§å°: {len(response.text)} å­—ç¬¦\")\n",
    "\n",
    "# æŸ¥æ‰¾å…³é”®ä¿¡æ¯\n",
    "lines = response.text.split('\\n')\n",
    "print(\"\\nåŒ…å«'å›ç­”äº'çš„è¡Œ:\")\n",
    "for i, line in enumerate(lines):\n",
    "    if 'å›ç­”äº' in line:\n",
    "        print(f\"è¡Œ {i}: {line.strip()[:100]}\")\n",
    "\n",
    "print(\"\\nè¯·å°† debug_output.txt æ–‡ä»¶çš„å‰1000å­—ç¬¦å‘ç»™æˆ‘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿è¡Œå¿«é€Ÿæµ‹è¯•...\n",
      "å¿«é€Ÿæµ‹è¯• - æŠ“å–ç¬¬ä¸€é¡µæ•°æ®...\n",
      "æ‰¾åˆ° 10 ä¸ªé—®é¢˜\n",
      "\n",
      "é—®é¢˜ 1:\n",
      "  æ ‡é¢˜: è¯·é—®åœ¨æœ€æ–°çš„å¼€æºæ¿€åŠ±è®¡åˆ’ä¸­ï¼ˆ2025å¹´11æœˆå‘å¸ƒï¼‰ï¼Œå¦‚ä½•ç†è§£å¼€æºè´¡çŒ®præ•°è¿™ä¸ªæ¦‚å¿µï¼Ÿ...\n",
      "  ç”¨æˆ·: HanqinWu\n",
      "  æ—¶é—´: Dec 2, 2025\n",
      "  æµè§ˆ: 25, ç‚¹èµ: 0, å›ç­”: 1\n",
      "\n",
      "é—®é¢˜ 2:\n",
      "  æ ‡é¢˜: å¯¹äºå·¥ä¸šçº§åˆ«çš„è½¯ä»¶ï¼Œæ˜¯é¼“åŠ±ä½¿ç”¨å¼€æºè½¯ä»¶è¿˜æ˜¯åå¯¹ä½¿ç”¨å¼€æºè½¯ä»¶ï¼Ÿ...\n",
      "  ç”¨æˆ·: HanqinWu\n",
      "  æ—¶é—´: Nov 19, 2025\n",
      "  æµè§ˆ: 7, ç‚¹èµ: 0, å›ç­”: 1\n",
      "\n",
      "é—®é¢˜ 3:\n",
      "  æ ‡é¢˜: æœ€è¿‘æœ‰æœ‰å…³å¤©å·¥å¼€ç‰©åŸºé‡‘ä¼šæˆç«‹å·¥ä¸šè½¯ä»¶å§”å‘˜ä¼šçš„åŠ¨æ€å—ï¼ŸåŒ…æ‹¬ç›¸å…³ä¿¡æ¯ä¹Ÿè¡Œã€‚...\n",
      "  ç”¨æˆ·: HanqinWu\n",
      "  æ—¶é—´: Nov 2, 2025\n",
      "  æµè§ˆ: 8, ç‚¹èµ: 0, å›ç­”: 1\n",
      "\n",
      "å¿«é€Ÿæµ‹è¯•æˆåŠŸï¼Œå¼€å§‹å®Œæ•´æŠ“å–...\n",
      "======================================================================\n",
      "å¤©å·¥å¼€ç‰©é—®ç­”ç«™æ•°æ®æŠ“å–å·¥å…·\n",
      "======================================================================\n",
      "ç›®æ ‡ç½‘ç«™: https://answer.chancefoundation.org.cn/\n",
      "è¾“å‡ºè·¯å¾„: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data.csv\n",
      "\n",
      "å¼€å§‹æŠ“å–æ•°æ®...\n",
      "æ­£åœ¨æŠ“å–ç¬¬ 1 é¡µ: https://answer.chancefoundation.org.cn/questions\n",
      "ç¬¬ 1 é¡µæ‰¾åˆ° 10 ä¸ªé—®é¢˜\n",
      "\n",
      "======================================================================\n",
      "æ•°æ®åˆ†ææŠ¥å‘Š\n",
      "======================================================================\n",
      "âœ… æˆåŠŸæŠ“å– 10 ä¸ªé—®é¢˜\n",
      "\n",
      "ğŸ“Š åŸºç¡€ç»Ÿè®¡:\n",
      "  æ€»æµè§ˆæ•°: 77\n",
      "  æ€»ç‚¹èµæ•°: 0\n",
      "  æ€»å›ç­”æ•°: 10\n",
      "  æ€»ç”¨æˆ·å£°æœ›: 30\n",
      "\n",
      "ğŸ“ˆ å¹³å‡å€¼:\n",
      "  å¹³å‡æµè§ˆæ•°: 7.7\n",
      "  å¹³å‡ç‚¹èµæ•°: 0.0\n",
      "  å¹³å‡å›ç­”æ•°: 1.0\n",
      "\n",
      "ğŸ”¥ æœ€å—æ¬¢è¿çš„é—®é¢˜ï¼ˆæŒ‰æµè§ˆæ•°ï¼‰:\n",
      "  1. è¯·é—®åœ¨æœ€æ–°çš„å¼€æºæ¿€åŠ±è®¡åˆ’ä¸­ï¼ˆ2025å¹´11æœˆå‘å¸ƒï¼‰ï¼Œå¦‚ä½•ç†è§£å¼€æºè´¡çŒ®præ•°è¿™ä¸ªæ¦‚å¿µï¼Ÿ...\n",
      "     æµè§ˆ: 25æ¬¡, ç‚¹èµ: 0ä¸ª, æ—¶é—´: Dec 2, 2025\n",
      "  2. è¯·é—®åŸºé‡‘ä¼šè¿™è¾¹æœ‰æ²¡æœ‰é¡¹ç›®åœ¨æ‹›äººåšï¼Ÿæƒ³å‚ä¸ä¸€äº›å¼€æºé¡¹ç›®...\n",
      "     æµè§ˆ: 9æ¬¡, ç‚¹èµ: 0ä¸ª, æ—¶é—´: Nov 2, 2025\n",
      "  3. æœ€è¿‘æœ‰æœ‰å…³å¤©å·¥å¼€ç‰©åŸºé‡‘ä¼šæˆç«‹å·¥ä¸šè½¯ä»¶å§”å‘˜ä¼šçš„åŠ¨æ€å—ï¼ŸåŒ…æ‹¬ç›¸å…³ä¿¡æ¯ä¹Ÿè¡Œã€‚...\n",
      "     æµè§ˆ: 8æ¬¡, ç‚¹èµ: 0ä¸ª, æ—¶é—´: Nov 2, 2025\n",
      "  4. ä»€ä¹ˆæ˜¯SIGï¼Œåœ¨å­¦æ ¡æ²¡æœ‰ç›¸å…³æ”¿ç­–æ¿€åŠ±çš„æƒ…å†µä¸‹ï¼Œæ€ä¹ˆå…ˆå»åšè¿™ä¸ªå¼€æºçš„ç”Ÿæ€ï¼Ÿ...\n",
      "     æµè§ˆ: 8æ¬¡, ç‚¹èµ: 0ä¸ª, æ—¶é—´: Oct 27, 2025\n",
      "  5. å¯¹äºå·¥ä¸šçº§åˆ«çš„è½¯ä»¶ï¼Œæ˜¯é¼“åŠ±ä½¿ç”¨å¼€æºè½¯ä»¶è¿˜æ˜¯åå¯¹ä½¿ç”¨å¼€æºè½¯ä»¶ï¼Ÿ...\n",
      "     æµè§ˆ: 7æ¬¡, ç‚¹èµ: 0ä¸ª, æ—¶é—´: Nov 19, 2025\n",
      "\n",
      "ğŸ‘¥ ç”¨æˆ·è´¡çŒ®ç»Ÿè®¡:\n",
      "          æµè§ˆæ•°  ç‚¹èµæ•°  å›ç­”æ•°  æé—®æ•°\n",
      "æé—®ç”¨æˆ·                        \n",
      "HanqinWu   77    0   10   10\n",
      "\n",
      "ğŸ·ï¸ çƒ­é—¨æ ‡ç­¾:\n",
      "  å¼€æºæ•™è‚²: 5æ¬¡\n",
      "  å¼€æºæ¿€åŠ±è®¡åˆ’: 1æ¬¡\n",
      "  å¼€æºè®¸å¯è¯: 1æ¬¡\n",
      "  åŸºé‡‘ä¼šæ´»åŠ¨: 1æ¬¡\n",
      "  å¼€æºæ³•è§„: 1æ¬¡\n",
      "  å¼€æºçƒ­ç‚¹é—®é¢˜: 1æ¬¡\n",
      "\n",
      "ğŸ“… æœˆåº¦ç»Ÿè®¡:\n",
      "          æµè§ˆæ•°  ç‚¹èµæ•°  æé—®æ•°\n",
      "æœˆä»½                     \n",
      "Oct 2025   24    0    5\n",
      "Nov 2025   28    0    4\n",
      "Dec 2025   25    0    1\n",
      "\n",
      "ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° CSV: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data.csv\n",
      "ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° Excel: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data.xlsx\n",
      "ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_report.txt\n",
      "\n",
      "ğŸ“‹ æ•°æ®é¢„è§ˆï¼ˆå‰10æ¡ï¼‰:\n",
      "             é—®é¢˜ID                                                             æ ‡é¢˜     æé—®ç”¨æˆ·  ç”¨æˆ·å£°æœ›         æé—®æ—¶é—´                     ç²¾ç¡®æ—¶é—´  ç‚¹èµæ•°  å›ç­”æ•°  æµè§ˆæ•°     æ ‡ç­¾                                                                                                                                                                                                                      é—®é¢˜é“¾æ¥                                                  ç”¨æˆ·é“¾æ¥       æœˆä»½\n",
      "10010000000000098                     è¯·é—®åœ¨æœ€æ–°çš„å¼€æºæ¿€åŠ±è®¡åˆ’ä¸­ï¼ˆ2025å¹´11æœˆå‘å¸ƒï¼‰ï¼Œå¦‚ä½•ç†è§£å¼€æºè´¡çŒ®præ•°è¿™ä¸ªæ¦‚å¿µï¼Ÿ HanqinWu     3  Dec 2, 2025 2025-12-02T08:07:28.000Z    0    1   25 å¼€æºæ¿€åŠ±è®¡åˆ’                      https://answer.chancefoundation.org.cn/questions/10010000000000098/qing-wen-zai-zui-xin-de-kai-yuan-ji-li-ji-hua-zhong-2025-nian-11-yue-fa-bu-ru-he-li-jie-kai-yuan-gong-xian-pr-shu-zhe-ge-gai-nian https://answer.chancefoundation.org.cn/users/hanqinwu Dec 2025\n",
      "10010000000000096                                 å¯¹äºå·¥ä¸šçº§åˆ«çš„è½¯ä»¶ï¼Œæ˜¯é¼“åŠ±ä½¿ç”¨å¼€æºè½¯ä»¶è¿˜æ˜¯åå¯¹ä½¿ç”¨å¼€æºè½¯ä»¶ï¼Ÿ HanqinWu     3 Nov 19, 2025 2025-11-19T03:37:12.000Z    0    1    7  å¼€æºè®¸å¯è¯                                   https://answer.chancefoundation.org.cn/questions/10010000000000096/dui-yu-gong-ye-ji-bie-de-ruan-jian-shi-gu-li-shi-yong-kai-yuan-ruan-jian-hai-shi-fan-dui-shi-yong-kai-yuan-ruan-jian https://answer.chancefoundation.org.cn/users/hanqinwu Nov 2025\n",
      "10010000000000094                            æœ€è¿‘æœ‰æœ‰å…³å¤©å·¥å¼€ç‰©åŸºé‡‘ä¼šæˆç«‹å·¥ä¸šè½¯ä»¶å§”å‘˜ä¼šçš„åŠ¨æ€å—ï¼ŸåŒ…æ‹¬ç›¸å…³ä¿¡æ¯ä¹Ÿè¡Œã€‚ HanqinWu     3  Nov 2, 2025 2025-11-02T06:31:10.000Z    0    1    8  åŸºé‡‘ä¼šæ´»åŠ¨              https://answer.chancefoundation.org.cn/questions/10010000000000094/zui-jin-you-you-guan-tian-gong-kai-wu-ji-jin-hui-cheng-li-gong-ye-ruan-jian-wei-yuan-hui-de-dong-tai-ma-bao-kuo-xiang-guan-xin-xi-ye-xing https://answer.chancefoundation.org.cn/users/hanqinwu Nov 2025\n",
      "10010000000000092                                     è¯·é—®åŸºé‡‘ä¼šè¿™è¾¹æœ‰æ²¡æœ‰é¡¹ç›®åœ¨æ‹›äººåšï¼Ÿæƒ³å‚ä¸ä¸€äº›å¼€æºé¡¹ç›® HanqinWu     3  Nov 2, 2025 2025-11-02T06:28:46.000Z    0    1    9   å¼€æºæ•™è‚²                                               https://answer.chancefoundation.org.cn/questions/10010000000000092/qing-wen-ji-jin-hui-zhe-bian-you-mei-you-xiang-mu-zai-zhao-ren-zuo-xiang-can-yu-yi-xie-kai-yuan-xiang-mu https://answer.chancefoundation.org.cn/users/hanqinwu Nov 2025\n",
      "10010000000000090                            ä¸€ä¸ªä¼ä¸šè¦æˆç«‹ospoï¼Œå¦‚æœæ²¡æœ‰è¯´è¦å¼€æ”¾æºä»£ç ï¼Œè¿˜éœ€è¦æˆç«‹ospoå—ï¼Ÿ HanqinWu     3  Nov 1, 2025 2025-11-01T06:22:59.000Z    0    1    4   å¼€æºæ³•è§„                                                 https://answer.chancefoundation.org.cn/questions/10010000000000090/yi-ge-qi-ye-yao-cheng-li-ospo-ru-guo-mei-you-shuo-yao-kai-fang-yuan-dai-ma-hai-xu-yao-cheng-li-ospo-ma https://answer.chancefoundation.org.cn/users/hanqinwu Nov 2025\n",
      "10010000000000088 å¯¹å¾…æ„Ÿå…´è¶£çš„å¼€æºé¡¹ç›®ï¼ˆäº§å“ï¼‰ï¼Œç”±äºæœ¬èº«æ¯”è¾ƒéš¾ï¼Œæˆ‘æ²¡æœ‰åˆ†è¾¨ä¸€äº›æŠ€æœ¯ç»†èŠ‚èƒ½åŠ›ï¼Œè¯·é—®æ€ä¹ˆå»å­¦ä¹ ä»–ä»¬ï¼Ÿæˆ‘æ€•è¢«ç±»ä¼¼çš„å¼€æºé¡¹ç›®äººäº‘äº¦äº‘ã€‚ HanqinWu     3 Oct 27, 2025 2025-10-27T13:20:24.000Z    0    1    5   å¼€æºæ•™è‚² https://answer.chancefoundation.org.cn/questions/10010000000000088/dui-dai-gan-xing-qu-de-kai-yuan-xiang-mu-chan-pin-you-yu-ben-shen-bi-jiao-nan-wo-mei-you-fen-bian-yi-xie-ji-shu-xi-jie-neng-li-qing-wen-zen-me-qu-xue- https://answer.chancefoundation.org.cn/users/hanqinwu Oct 2025\n",
      "10010000000000083                                        å¦‚ä½•æˆç«‹ä¸€ä¸ªæ–°çš„SIGï¼Œæˆ‘ä»¬æ€ä¹ˆå»è¿ä½œSIGï¼Ÿ HanqinWu     3 Oct 27, 2025 2025-10-27T12:53:59.000Z    0    1    2   å¼€æºæ•™è‚²                                                                                           https://answer.chancefoundation.org.cn/questions/10010000000000083/ru-he-cheng-li-yi-ge-xin-de-sig-wo-men-zen-me-qu-yun-zuo-sig https://answer.chancefoundation.org.cn/users/hanqinwu Oct 2025\n",
      "10010000000000082     ä»€ä¹ˆæ˜¯å¼€å‘è€…è¡Œä¸ºæ•°æ®æ´å¯Ÿï¼Œopendiggerèƒ½ä¸èƒ½åšè¿™ä¸ªäº‹æƒ…ï¼Œä¼ä¸šç¤¾åŒºèƒ½å¤Ÿç”¨è¿™ä¸ªæ•°æ®åšä»€ä¹ˆï¼Ÿèƒ½ç»™å¼€æºæ•™è‚²ä»€ä¹ˆå¯ç¤ºï¼Ÿ HanqinWu     3 Oct 27, 2025 2025-10-27T12:53:08.000Z    0    1    4   å¼€æºæ•™è‚² https://answer.chancefoundation.org.cn/questions/10010000000000082/shen-me-shi-kai-fa-zhe-xing-wei-shu-ju-dong-cha-opendigger-neng-bu-neng-zuo-zhe-ge-shi-qing-qi-ye-she-qu-neng-gou-yong-zhe-ge-shu-ju-zuo-shen-me-neng- https://answer.chancefoundation.org.cn/users/hanqinwu Oct 2025\n",
      "10010000000000080                    discuss:å°±2025å¹´è¯ºè´å°”ç»æµå­¦å¥–æé—®ï¼Œè¯·é—®æ˜¯åˆ›æ–°é©±åŠ¨å’Œç»æµå¢é•¿çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ HanqinWu     3 Oct 27, 2025 2025-10-27T12:51:35.000Z    0    1    5 å¼€æºçƒ­ç‚¹é—®é¢˜                 https://answer.chancefoundation.org.cn/questions/10010000000000080/discuss-jiu-2025-nian-nuo-bei-er-jing-ji-xue-jiang-ti-wen-qing-wen-shi-chuang-xin-qu-dong-he-jing-ji-zeng-zhang-de-guan-xi-shi-shen-me https://answer.chancefoundation.org.cn/users/hanqinwu Oct 2025\n",
      "10010000000000079                           ä»€ä¹ˆæ˜¯SIGï¼Œåœ¨å­¦æ ¡æ²¡æœ‰ç›¸å…³æ”¿ç­–æ¿€åŠ±çš„æƒ…å†µä¸‹ï¼Œæ€ä¹ˆå…ˆå»åšè¿™ä¸ªå¼€æºçš„ç”Ÿæ€ï¼Ÿ HanqinWu     3 Oct 27, 2025 2025-10-27T12:49:37.000Z    0    1    8   å¼€æºæ•™è‚²                       https://answer.chancefoundation.org.cn/questions/10010000000000079/shen-me-shi-sig-zai-xue-xiao-mei-you-xiang-guan-zheng-ce-ji-li-de-qing-kuang-xia-zen-me-xian-qu-zuo-zhe-ge-kai-yuan-de-sheng-tai https://answer.chancefoundation.org.cn/users/hanqinwu Oct 2025\n",
      "\n",
      "âœ… ä»»åŠ¡å®Œæˆï¼\n",
      "   å…±æŠ“å– 10 ä¸ªé—®é¢˜\n",
      "   æ•°æ®å·²ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„\n"
     ]
    }
   ],
   "source": [
    "å°è¯•çˆ¬å–ç¬¬ä¸€é¡µæ•°æ®\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "def fetch_all_questions(base_url, max_pages=10):\n",
    "    \"\"\"\n",
    "    æŠ“å–æ‰€æœ‰é¡µé¢é—®é¢˜æ•°æ®\n",
    "    \"\"\"\n",
    "    all_questions = []\n",
    "    page = 1\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        # æ„å»ºé¡µé¢URL\n",
    "        if page == 1:\n",
    "            url = base_url + \"questions\"\n",
    "        else:\n",
    "            url = f\"{base_url}questions?page={page}\"\n",
    "        \n",
    "        print(f\"æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ: {url}\")\n",
    "        \n",
    "        try:\n",
    "            # å‘é€è¯·æ±‚\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥: {response.status_code}\")\n",
    "                break\n",
    "            \n",
    "            # è§£æé¡µé¢\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # æŸ¥æ‰¾é—®é¢˜åˆ—è¡¨\n",
    "            question_items = soup.find_all('div', class_='list-group-item')\n",
    "            \n",
    "            if not question_items:\n",
    "                print(f\"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°é—®é¢˜\")\n",
    "                break\n",
    "            \n",
    "            print(f\"ç¬¬ {page} é¡µæ‰¾åˆ° {len(question_items)} ä¸ªé—®é¢˜\")\n",
    "            \n",
    "            # æå–æ¯ä¸ªé—®é¢˜çš„æ•°æ®\n",
    "            for item in question_items:\n",
    "                question_data = extract_question_data(item)\n",
    "                if question_data:\n",
    "                    all_questions.append(question_data)\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ\n",
    "            next_page = soup.find('a', string='Next')\n",
    "            if not next_page or page >= max_pages:\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "            time.sleep(1)  # ç¤¼è²Œç­‰å¾…\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"æŠ“å–ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_questions\n",
    "\n",
    "def extract_question_data(question_item):\n",
    "    \"\"\"\n",
    "    ä»å•ä¸ªé—®é¢˜å…ƒç´ ä¸­æå–æ•°æ®\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # æå–é—®é¢˜æ ‡é¢˜\n",
    "        title_elem = question_item.find('a', class_='link-dark')\n",
    "        title = title_elem.text.strip() if title_elem else \"æœªçŸ¥æ ‡é¢˜\"\n",
    "        \n",
    "        # æå–é—®é¢˜é“¾æ¥\n",
    "        question_link = title_elem['href'] if title_elem else \"\"\n",
    "        \n",
    "        # æå–ç”¨æˆ·ä¿¡æ¯\n",
    "        user_elem = question_item.find('a', href=re.compile(r'/users/'))\n",
    "        user = user_elem.text.strip() if user_elem else \"åŒ¿åç”¨æˆ·\"\n",
    "        \n",
    "        # æå–å£°æœ›å€¼\n",
    "        reputation_elem = question_item.find('span', title='Reputation')\n",
    "        reputation = int(reputation_elem.text.strip()) if reputation_elem else 0\n",
    "        \n",
    "        # æå–æé—®æ—¶é—´ - æŸ¥æ‰¾timeæ ‡ç­¾\n",
    "        time_elem = question_item.find('time')\n",
    "        asked_time = time_elem.text.strip().replace('asked', '').strip() if time_elem else \"æœªçŸ¥æ—¶é—´\"\n",
    "        \n",
    "        # æå–ç‚¹èµæ•° - æŸ¥æ‰¾åŒ…å«hand-thumbs-upçš„iæ ‡ç­¾\n",
    "        likes_elem = question_item.find('i', class_='bi-hand-thumbs-up-fill')\n",
    "        if likes_elem:\n",
    "            likes = int(likes_elem.find_next('em').text.strip())\n",
    "        else:\n",
    "            likes = 0\n",
    "        \n",
    "        # æå–å›ç­”æ•°\n",
    "        answers_elem = question_item.find('i', class_='bi-chat-square-text-fill')\n",
    "        if answers_elem:\n",
    "            answers = int(answers_elem.find_next('em').text.strip())\n",
    "        else:\n",
    "            answers = 0\n",
    "        \n",
    "        # æå–æµè§ˆæ•°\n",
    "        views_elem = question_item.find('i', class_='bi-eye-fill')\n",
    "        if views_elem:\n",
    "            views = int(views_elem.find_next('em').text.strip())\n",
    "        else:\n",
    "            views = 0\n",
    "        \n",
    "        # æå–æ ‡ç­¾\n",
    "        tags = []\n",
    "        tag_elems = question_item.find_all('a', class_='badge-tag')\n",
    "        for tag_elem in tag_elems:\n",
    "            tag_text = tag_elem.find('span').text.strip()\n",
    "            tags.append(tag_text)\n",
    "        \n",
    "        # æå–datetimeå±æ€§ä¸­çš„ç²¾ç¡®æ—¶é—´\n",
    "        datetime_str = \"\"\n",
    "        if time_elem and 'datetime' in time_elem.attrs:\n",
    "            datetime_str = time_elem['datetime']\n",
    "        \n",
    "        # æå–é—®é¢˜ID\n",
    "        question_id = \"\"\n",
    "        if question_link:\n",
    "            match = re.search(r'/questions/(\\d+)', question_link)\n",
    "            if match:\n",
    "                question_id = match.group(1)\n",
    "        \n",
    "        return {\n",
    "            'é—®é¢˜ID': question_id,\n",
    "            'æ ‡é¢˜': title,\n",
    "            'æé—®ç”¨æˆ·': user,\n",
    "            'ç”¨æˆ·å£°æœ›': reputation,\n",
    "            'æé—®æ—¶é—´': asked_time,\n",
    "            'ç²¾ç¡®æ—¶é—´': datetime_str,\n",
    "            'ç‚¹èµæ•°': likes,\n",
    "            'å›ç­”æ•°': answers,\n",
    "            'æµè§ˆæ•°': views,\n",
    "            'æ ‡ç­¾': ', '.join(tags),\n",
    "            'é—®é¢˜é“¾æ¥': f\"https://answer.chancefoundation.org.cn{question_link}\" if question_link else \"\",\n",
    "            'ç”¨æˆ·é“¾æ¥': f\"https://answer.chancefoundation.org.cn{user_elem['href']}\" if user_elem else \"\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"æå–é—®é¢˜æ•°æ®æ—¶å‡ºé”™: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_questions_data(questions):\n",
    "    \"\"\"\n",
    "    åˆ†æé—®é¢˜æ•°æ®\n",
    "    \"\"\"\n",
    "    if not questions:\n",
    "        print(\"æ²¡æœ‰æ•°æ®å¯åˆ†æ\")\n",
    "        return None\n",
    "    \n",
    "    # è½¬æ¢ä¸ºDataFrame\n",
    "    df = pd.DataFrame(questions)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"æ•°æ®åˆ†ææŠ¥å‘Š\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"âœ… æˆåŠŸæŠ“å– {len(df)} ä¸ªé—®é¢˜\")\n",
    "    \n",
    "    # åŸºç¡€ç»Ÿè®¡\n",
    "    print(f\"\\nğŸ“Š åŸºç¡€ç»Ÿè®¡:\")\n",
    "    print(f\"  æ€»æµè§ˆæ•°: {df['æµè§ˆæ•°'].sum():,}\")\n",
    "    print(f\"  æ€»ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].sum():,}\")\n",
    "    print(f\"  æ€»å›ç­”æ•°: {df['å›ç­”æ•°'].sum():,}\")\n",
    "    print(f\"  æ€»ç”¨æˆ·å£°æœ›: {df['ç”¨æˆ·å£°æœ›'].sum():,}\")\n",
    "    \n",
    "    # å¹³å‡å€¼\n",
    "    print(f\"\\nğŸ“ˆ å¹³å‡å€¼:\")\n",
    "    print(f\"  å¹³å‡æµè§ˆæ•°: {df['æµè§ˆæ•°'].mean():.1f}\")\n",
    "    print(f\"  å¹³å‡ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].mean():.1f}\")\n",
    "    print(f\"  å¹³å‡å›ç­”æ•°: {df['å›ç­”æ•°'].mean():.1f}\")\n",
    "    \n",
    "    # æœ€å—æ¬¢è¿çš„é—®é¢˜\n",
    "    if len(df) > 0:\n",
    "        top_views = df.nlargest(5, 'æµè§ˆæ•°')[['æ ‡é¢˜', 'æµè§ˆæ•°', 'ç‚¹èµæ•°', 'æé—®æ—¶é—´']]\n",
    "        print(f\"\\nğŸ”¥ æœ€å—æ¬¢è¿çš„é—®é¢˜ï¼ˆæŒ‰æµè§ˆæ•°ï¼‰:\")\n",
    "        for i, (_, row) in enumerate(top_views.iterrows(), 1):\n",
    "            print(f\"  {i}. {row['æ ‡é¢˜'][:50]}...\")\n",
    "            print(f\"     æµè§ˆ: {row['æµè§ˆæ•°']}æ¬¡, ç‚¹èµ: {row['ç‚¹èµæ•°']}ä¸ª, æ—¶é—´: {row['æé—®æ—¶é—´']}\")\n",
    "    \n",
    "    # ç”¨æˆ·åˆ†æ\n",
    "    if 'æé—®ç”¨æˆ·' in df.columns:\n",
    "        user_stats = df.groupby('æé—®ç”¨æˆ·').agg({\n",
    "            'æµè§ˆæ•°': 'sum',\n",
    "            'ç‚¹èµæ•°': 'sum',\n",
    "            'å›ç­”æ•°': 'sum',\n",
    "            'æ ‡é¢˜': 'count'\n",
    "        }).rename(columns={'æ ‡é¢˜': 'æé—®æ•°'}).sort_values('æé—®æ•°', ascending=False)\n",
    "        \n",
    "        print(f\"\\nğŸ‘¥ ç”¨æˆ·è´¡çŒ®ç»Ÿè®¡:\")\n",
    "        print(user_stats.head(10).to_string())\n",
    "    \n",
    "    # æ ‡ç­¾åˆ†æ\n",
    "    if 'æ ‡ç­¾' in df.columns:\n",
    "        # å±•å¼€æ ‡ç­¾æ•°æ®\n",
    "        all_tags = []\n",
    "        for tags_str in df['æ ‡ç­¾']:\n",
    "            if tags_str:\n",
    "                tags = [tag.strip() for tag in tags_str.split(',')]\n",
    "                all_tags.extend(tags)\n",
    "        \n",
    "        if all_tags:\n",
    "            from collections import Counter\n",
    "            tag_counts = Counter(all_tags)\n",
    "            \n",
    "            print(f\"\\nğŸ·ï¸ çƒ­é—¨æ ‡ç­¾:\")\n",
    "            for tag, count in tag_counts.most_common(10):\n",
    "                print(f\"  {tag}: {count}æ¬¡\")\n",
    "    \n",
    "    # æ—¶é—´åˆ†æï¼ˆå¦‚æœå¯èƒ½ï¼‰\n",
    "    if 'æé—®æ—¶é—´' in df.columns:\n",
    "        # æå–æœˆä»½ä¿¡æ¯\n",
    "        try:\n",
    "            df['æœˆä»½'] = df['æé—®æ—¶é—´'].apply(extract_month)\n",
    "            monthly_stats = df.groupby('æœˆä»½').agg({\n",
    "                'æµè§ˆæ•°': 'sum',\n",
    "                'ç‚¹èµæ•°': 'sum',\n",
    "                'æ ‡é¢˜': 'count'\n",
    "            }).rename(columns={'æ ‡é¢˜': 'æé—®æ•°'}).sort_index(ascending=False)\n",
    "            \n",
    "            print(f\"\\nğŸ“… æœˆåº¦ç»Ÿè®¡:\")\n",
    "            print(monthly_stats.to_string())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_month(time_str):\n",
    "    \"\"\"ä»æ—¶é—´å­—ç¬¦ä¸²ä¸­æå–æœˆä»½\"\"\"\n",
    "    try:\n",
    "        # åŒ¹é… \"Dec 2, 2025\" æˆ–ç±»ä¼¼æ ¼å¼\n",
    "        match = re.search(r'(\\w+)\\s+\\d+,\\s+(\\d{4})', time_str)\n",
    "        if match:\n",
    "            return f\"{match.group(1)} {match.group(2)}\"\n",
    "        \n",
    "        # åŒ¹é…ä¸­æ–‡æ ¼å¼\n",
    "        match = re.search(r'(\\d{4})å¹´(\\d{1,2})æœˆ', time_str)\n",
    "        if match:\n",
    "            months_en = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "            month_num = int(match.group(2))\n",
    "            if 1 <= month_num <= 12:\n",
    "                return f\"{months_en[month_num-1]} {match.group(1)}\"\n",
    "        \n",
    "        return time_str[:7]\n",
    "    except:\n",
    "        return \"æœªçŸ¥æœˆä»½\"\n",
    "\n",
    "def save_data(df, output_path):\n",
    "    \"\"\"\n",
    "    ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"æ²¡æœ‰æ•°æ®å¯ä¿å­˜\")\n",
    "        return\n",
    "    \n",
    "    # ä¿å­˜ä¸ºCSV\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° CSV: {output_path}\")\n",
    "    \n",
    "    # ä¿å­˜ä¸ºExcel\n",
    "    try:\n",
    "        excel_path = output_path.replace('.csv', '.xlsx')\n",
    "        df.to_excel(excel_path, index=False)\n",
    "        print(f\"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° Excel: {excel_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Excelä¿å­˜å¤±è´¥: {e}\")\n",
    "    \n",
    "    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š\n",
    "    report_path = output_path.replace('.csv', '_report.txt')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"å¤©å·¥å¼€ç‰©é—®ç­”ç«™æ•°æ®åˆ†ææŠ¥å‘Š\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"æ•°æ®æ¥æº: https://answer.chancefoundation.org.cn/\\n\")\n",
    "        f.write(f\"æ€»é—®é¢˜æ•°: {len(df)}\\n\")\n",
    "        f.write(f\"æ€»æµè§ˆæ•°: {df['æµè§ˆæ•°'].sum()}\\n\")\n",
    "        f.write(f\"æ€»ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].sum()}\\n\")\n",
    "        f.write(f\"æ€»å›ç­”æ•°: {df['å›ç­”æ•°'].sum()}\\n\")\n",
    "        \n",
    "        # çƒ­é—¨é—®é¢˜\n",
    "        f.write(\"\\nçƒ­é—¨é—®é¢˜:\\n\")\n",
    "        top_5 = df.nlargest(5, 'æµè§ˆæ•°')\n",
    "        for i, (_, row) in enumerate(top_5.iterrows(), 1):\n",
    "            f.write(f\"{i}. {row['æ ‡é¢˜']}\\n\")\n",
    "            f.write(f\"   æµè§ˆ: {row['æµè§ˆæ•°']}, ç‚¹èµ: {row['ç‚¹èµæ•°']}, å›ç­”: {row['å›ç­”æ•°']}, æ—¶é—´: {row['æé—®æ—¶é—´']}\\n\")\n",
    "        \n",
    "        # è¯¦ç»†æ•°æ®\n",
    "        f.write(\"\\nè¯¦ç»†æ•°æ®:\\n\")\n",
    "        f.write(df.to_string(index=False))\n",
    "    \n",
    "    print(f\"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½æ•°\n",
    "    \"\"\"\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"å¤©å·¥å¼€ç‰©é—®ç­”ç«™æ•°æ®æŠ“å–å·¥å…·\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    base_url = \"https://answer.chancefoundation.org.cn/\"\n",
    "    output_path = r\"D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data.csv\"\n",
    "    \n",
    "    print(f\"ç›®æ ‡ç½‘ç«™: {base_url}\")\n",
    "    print(f\"è¾“å‡ºè·¯å¾„: {output_path}\")\n",
    "    \n",
    "    # æŠ“å–æ•°æ®\n",
    "    print(\"\\nå¼€å§‹æŠ“å–æ•°æ®...\")\n",
    "    questions = fetch_all_questions(base_url, max_pages=4)\n",
    "    \n",
    "    if questions:\n",
    "        # åˆ†ææ•°æ®\n",
    "        df = analyze_questions_data(questions)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # ä¿å­˜æ•°æ®\n",
    "            save_data(df, output_path)\n",
    "            \n",
    "            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ\n",
    "            print(f\"\\nğŸ“‹ æ•°æ®é¢„è§ˆï¼ˆå‰10æ¡ï¼‰:\")\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            pd.set_option('display.max_colwidth', 50)\n",
    "            print(df.head(10).to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nâœ… ä»»åŠ¡å®Œæˆï¼\")\n",
    "            print(f\"   å…±æŠ“å– {len(df)} ä¸ªé—®é¢˜\")\n",
    "            print(f\"   æ•°æ®å·²ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„\")\n",
    "        else:\n",
    "            print(\"\\nâŒ æ•°æ®æå–å¤±è´¥\")\n",
    "    else:\n",
    "        print(\"\\nâŒ æ²¡æœ‰æŠ“åˆ°ä»»ä½•æ•°æ®\")\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"\n",
    "    å¿«é€Ÿæµ‹è¯•å‡½æ•° - æŠ“å–ç¬¬ä¸€é¡µæ•°æ®\n",
    "    \"\"\"\n",
    "    print(\"å¿«é€Ÿæµ‹è¯• - æŠ“å–ç¬¬ä¸€é¡µæ•°æ®...\")\n",
    "    \n",
    "    url = \"https://answer.chancefoundation.org.cn/questions\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # æŸ¥æ‰¾é—®é¢˜åˆ—è¡¨\n",
    "            question_items = soup.find_all('div', class_='list-group-item')\n",
    "            print(f\"æ‰¾åˆ° {len(question_items)} ä¸ªé—®é¢˜\")\n",
    "            \n",
    "            # æå–å‰3ä¸ªé—®é¢˜ä½œä¸ºç¤ºä¾‹\n",
    "            for i, item in enumerate(question_items[:3], 1):\n",
    "                data = extract_question_data(item)\n",
    "                if data:\n",
    "                    print(f\"\\né—®é¢˜ {i}:\")\n",
    "                    print(f\"  æ ‡é¢˜: {data['æ ‡é¢˜'][:50]}...\")\n",
    "                    print(f\"  ç”¨æˆ·: {data['æé—®ç”¨æˆ·']}\")\n",
    "                    print(f\"  æ—¶é—´: {data['æé—®æ—¶é—´']}\")\n",
    "                    print(f\"  æµè§ˆ: {data['æµè§ˆæ•°']}, ç‚¹èµ: {data['ç‚¹èµæ•°']}, å›ç­”: {data['å›ç­”æ•°']}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"è¯·æ±‚å¤±è´¥: {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"æµ‹è¯•å¤±è´¥: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    \n",
    "    # é¦–å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•\n",
    "    print(\"è¿è¡Œå¿«é€Ÿæµ‹è¯•...\")\n",
    "    if quick_test():\n",
    "        print(\"\\nå¿«é€Ÿæµ‹è¯•æˆåŠŸï¼Œå¼€å§‹å®Œæ•´æŠ“å–...\")\n",
    "        main()\n",
    "    else:\n",
    "        print(\"\\nå¿«é€Ÿæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç½‘ç«™è®¿é—®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯•ç½‘ç«™è¿æ¥...\n",
      "âœ… ç½‘ç«™å¯æ­£å¸¸è®¿é—®\n",
      "======================================================================\n",
      "å¤©å·¥å¼€ç‰©é—®ç­”ç«™ - å®Œæ•´æ•°æ®æŠ“å–ä¸åˆ†æç³»ç»Ÿ\n",
      "======================================================================\n",
      "ç›®æ ‡ç½‘ç«™: https://answer.chancefoundation.org.cn\n",
      "è¾“å‡ºæ–‡ä»¶: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_full.*\n",
      "æœ€å¤§é¡µæ•°: 20\n",
      "\n",
      "======================================================================\n",
      "å¼€å§‹æŠ“å–æ•°æ®...\n",
      "å¼€å§‹æŠ“å–æ•°æ®ï¼Œæœ€å¤š 20 é¡µ...\n",
      "------------------------------------------------------------\n",
      "æŠ“å–ç¬¬ 1 é¡µ: https://answer.chancefoundation.org.cn/questions\n",
      "ç¬¬ 1 é¡µ: æŠ“åˆ° 10 ä¸ªé—®é¢˜ï¼Œç´¯è®¡ 10 ä¸ª\n",
      "æŠ“å–ç¬¬ 2 é¡µ: https://answer.chancefoundation.org.cn/questions?page=2\n",
      "ç¬¬ 2 é¡µ: æŠ“åˆ° 10 ä¸ªé—®é¢˜ï¼Œç´¯è®¡ 20 ä¸ª\n",
      "æŠ“å–ç¬¬ 3 é¡µ: https://answer.chancefoundation.org.cn/questions?page=3\n",
      "ç¬¬ 3 é¡µ: æŠ“åˆ° 10 ä¸ªé—®é¢˜ï¼Œç´¯è®¡ 30 ä¸ª\n",
      "æŠ“å–ç¬¬ 4 é¡µ: https://answer.chancefoundation.org.cn/questions?page=4\n",
      "ç¬¬ 4 é¡µ: æŠ“åˆ° 9 ä¸ªé—®é¢˜ï¼Œç´¯è®¡ 39 ä¸ª\n",
      "æ²¡æœ‰ä¸‹ä¸€é¡µäº†ï¼ŒæŠ“å–å®Œæˆ\n",
      "\n",
      "æŠ“å–å®Œæˆï¼å…±æŠ“å– 39 ä¸ªé—®é¢˜\n",
      "\n",
      "âœ… æŠ“å–å®Œæˆï¼å…±è·å¾— 39 æ¡æ•°æ®\n",
      "\n",
      "======================================================================\n",
      "æ•°æ®åˆ†æä¸­...\n",
      "======================================================================\n",
      "ğŸ“Š å…³é”®ç»Ÿè®¡:\n",
      "  æ€»é—®é¢˜æ•°: 39\n",
      "  æ€»æµè§ˆæ•°: 420\n",
      "  æ€»ç‚¹èµæ•°: 1\n",
      "  æ€»å›ç­”æ•°: 46\n",
      "  å¹³å‡æµè§ˆæ•°: 10.8\n",
      "\n",
      "ğŸ”¥ æœ€çƒ­é—¨é—®é¢˜:\n",
      "  1. æœ€è¿‘å›½åŠ¡é™¢ä¸“é—¨é¢å¸ƒäº†ä¸€ä¸ªæœ‰å…³å¼€æºçš„æ”¿ç­–ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆç†è§£ï¼Œä¹Ÿå°±æ˜¯æˆ‘æƒ³é—®ï¼šå…³äºâ€œä¿ƒè¿›äººå·¥æ™ºèƒ½å¼€æºç”Ÿæ€ç¹è£â€æ”¿ç­–çš„é€šä¿—è§£è¯»...\n",
      "     æµè§ˆ: 39æ¬¡, æ—¶é—´: Aug 28, 2025\n",
      "  2. è¯·é—®åœ¨æœ€æ–°çš„å¼€æºæ¿€åŠ±è®¡åˆ’ä¸­ï¼ˆ2025å¹´11æœˆå‘å¸ƒï¼‰ï¼Œå¦‚ä½•ç†è§£å¼€æºè´¡çŒ®præ•°è¿™ä¸ªæ¦‚å¿µï¼Ÿ...\n",
      "     æµè§ˆ: 25æ¬¡, æ—¶é—´: Dec 2, 2025\n",
      "  3. å¤©å·¥å¼€ç‰©åŸºé‡‘ä¼šå¹³æ—¶ä¼šå‚ä¸å“ªäº›æ´»åŠ¨ï¼Ÿæˆ–è€…è¿™æ ·è¯´å§ä¼šæœ‰å“ªäº›ä¼šè®®ï¼Œæˆ‘æƒ³å»æ‰¾ä½ ä»¬äº†è§£ä¸€ä¸‹...\n",
      "     æµè§ˆ: 25æ¬¡, æ—¶é—´: Aug 7, 2025\n",
      "\n",
      "ğŸ‘¥ ç”¨æˆ·ç»Ÿè®¡:\n",
      "  æ€»ç”¨æˆ·æ•°: 4\n",
      "  æœ€å¤šæé—®ç”¨æˆ·: HanqinWu\n",
      "\n",
      "======================================================================\n",
      "å¯¼å‡ºæ•°æ®...\n",
      "======================================================================\n",
      "âœ… CSVæ–‡ä»¶å·²ä¿å­˜: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_full.csv\n",
      "âœ… Excelæ–‡ä»¶å·²ä¿å­˜: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_full.xlsx\n",
      "âœ… JSONæ–‡ä»¶å·²ä¿å­˜: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_full.json\n",
      "âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_full_report.txt\n",
      "\n",
      "======================================================================\n",
      "æ•°æ®é¢„è§ˆ\n",
      "======================================================================\n",
      "æ•°æ®å½¢çŠ¶: 39 è¡Œ Ã— 14 åˆ—\n",
      "\n",
      "åˆ—å: é—®é¢˜ID, æ ‡é¢˜, æé—®ç”¨æˆ·, ç”¨æˆ·å£°æœ›, æé—®æ—¶é—´, ç²¾ç¡®æ—¶é—´, ç‚¹èµæ•°, å›ç­”æ•°, æµè§ˆæ•°, æ ‡ç­¾, é—®é¢˜é“¾æ¥, ç”¨æˆ·é“¾æ¥, æŠ“å–æ—¶é—´, æ¥æºé¡µç \n",
      "\n",
      "å‰5æ¡æ•°æ®:\n",
      "             é—®é¢˜ID                                         æ ‡é¢˜     æé—®ç”¨æˆ·  ç”¨æˆ·å£°æœ›         æé—®æ—¶é—´                     ç²¾ç¡®æ—¶é—´  ç‚¹èµæ•°  å›ç­”æ•°  æµè§ˆæ•°     æ ‡ç­¾                                                                                                                                                                                                         é—®é¢˜é“¾æ¥                                                  ç”¨æˆ·é“¾æ¥                æŠ“å–æ—¶é—´  æ¥æºé¡µç \n",
      "10010000000000098 è¯·é—®åœ¨æœ€æ–°çš„å¼€æºæ¿€åŠ±è®¡åˆ’ä¸­ï¼ˆ2025å¹´11æœˆå‘å¸ƒï¼‰ï¼Œå¦‚ä½•ç†è§£å¼€æºè´¡çŒ®præ•°è¿™ä¸ªæ¦‚å¿µï¼Ÿ HanqinWu     3  Dec 2, 2025 2025-12-02T08:07:28.000Z    0    1   25 å¼€æºæ¿€åŠ±è®¡åˆ’         https://answer.chancefoundation.org.cn/questions/10010000000000098/qing-wen-zai-zui-xin-de-kai-yuan-ji-li-ji-hua-zhong-2025-nian-11-yue-fa-bu-ru-he-li-jie-kai-yuan-gong-xian-pr-shu-zhe-ge-gai-nian https://answer.chancefoundation.org.cn/users/hanqinwu 2026-01-03 12:06:47     1\n",
      "10010000000000096             å¯¹äºå·¥ä¸šçº§åˆ«çš„è½¯ä»¶ï¼Œæ˜¯é¼“åŠ±ä½¿ç”¨å¼€æºè½¯ä»¶è¿˜æ˜¯åå¯¹ä½¿ç”¨å¼€æºè½¯ä»¶ï¼Ÿ HanqinWu     3 Nov 19, 2025 2025-11-19T03:37:12.000Z    0    1    7  å¼€æºè®¸å¯è¯                      https://answer.chancefoundation.org.cn/questions/10010000000000096/dui-yu-gong-ye-ji-bie-de-ruan-jian-shi-gu-li-shi-yong-kai-yuan-ruan-jian-hai-shi-fan-dui-shi-yong-kai-yuan-ruan-jian https://answer.chancefoundation.org.cn/users/hanqinwu 2026-01-03 12:06:47     1\n",
      "10010000000000094        æœ€è¿‘æœ‰æœ‰å…³å¤©å·¥å¼€ç‰©åŸºé‡‘ä¼šæˆç«‹å·¥ä¸šè½¯ä»¶å§”å‘˜ä¼šçš„åŠ¨æ€å—ï¼ŸåŒ…æ‹¬ç›¸å…³ä¿¡æ¯ä¹Ÿè¡Œã€‚ HanqinWu     3  Nov 2, 2025 2025-11-02T06:31:10.000Z    0    1    8  åŸºé‡‘ä¼šæ´»åŠ¨ https://answer.chancefoundation.org.cn/questions/10010000000000094/zui-jin-you-you-guan-tian-gong-kai-wu-ji-jin-hui-cheng-li-gong-ye-ruan-jian-wei-yuan-hui-de-dong-tai-ma-bao-kuo-xiang-guan-xin-xi-ye-xing https://answer.chancefoundation.org.cn/users/hanqinwu 2026-01-03 12:06:47     1\n",
      "10010000000000092                 è¯·é—®åŸºé‡‘ä¼šè¿™è¾¹æœ‰æ²¡æœ‰é¡¹ç›®åœ¨æ‹›äººåšï¼Ÿæƒ³å‚ä¸ä¸€äº›å¼€æºé¡¹ç›® HanqinWu     3  Nov 2, 2025 2025-11-02T06:28:46.000Z    0    1    9   å¼€æºæ•™è‚²                                  https://answer.chancefoundation.org.cn/questions/10010000000000092/qing-wen-ji-jin-hui-zhe-bian-you-mei-you-xiang-mu-zai-zhao-ren-zuo-xiang-can-yu-yi-xie-kai-yuan-xiang-mu https://answer.chancefoundation.org.cn/users/hanqinwu 2026-01-03 12:06:47     1\n",
      "10010000000000090        ä¸€ä¸ªä¼ä¸šè¦æˆç«‹ospoï¼Œå¦‚æœæ²¡æœ‰è¯´è¦å¼€æ”¾æºä»£ç ï¼Œè¿˜éœ€è¦æˆç«‹ospoå—ï¼Ÿ HanqinWu     3  Nov 1, 2025 2025-11-01T06:22:59.000Z    0    1    4   å¼€æºæ³•è§„                                    https://answer.chancefoundation.org.cn/questions/10010000000000090/yi-ge-qi-ye-yao-cheng-li-ospo-ru-guo-mei-you-shuo-yao-kai-fang-yuan-dai-ma-hai-xu-yao-cheng-li-ospo-ma https://answer.chancefoundation.org.cn/users/hanqinwu 2026-01-03 12:06:47     1\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ ä»»åŠ¡å®Œæˆï¼\n",
      "======================================================================\n",
      "ğŸ“ ä¸»è¦æ•°æ®æ–‡ä»¶: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_full.csv\n",
      "ğŸ“„ è¯¦ç»†æŠ¥å‘Šæ–‡ä»¶: D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_full_report.txt\n",
      "\n",
      "ğŸ’¡ æç¤º:\n",
      "  - ä½¿ç”¨ Excel æ‰“å¼€ .xlsx æ–‡ä»¶è¿›è¡Œæ•°æ®åˆ†æ\n",
      "  - ä½¿ç”¨ æ–‡æœ¬ç¼–è¾‘å™¨ æ‰“å¼€ _report.txt æŸ¥çœ‹è¯¦ç»†åˆ†æ\n",
      "  - å…±æŠ“å– 39 æ¡å®Œæ•´æ•°æ®\n"
     ]
    }
   ],
   "source": [
    "#å°è¯•çˆ¬å–æ‰€æœ‰é—®ç­”æ•°æ®\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class AnswerSiteCrawler:\n",
    "    \"\"\"å¤©å·¥å¼€ç‰©é—®ç­”ç«™å®Œæ•´æ•°æ®æŠ“å–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        \n",
    "    def fetch_all_questions(self, max_pages: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        æŠ“å–æ‰€æœ‰é¡µé¢é—®é¢˜æ•°æ®\n",
    "        \n",
    "        Args:\n",
    "            max_pages: æœ€å¤§æŠ“å–é¡µæ•°\n",
    "            \n",
    "        Returns:\n",
    "            é—®é¢˜æ•°æ®åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        all_questions = []\n",
    "        page = 1\n",
    "        \n",
    "        print(f\"å¼€å§‹æŠ“å–æ•°æ®ï¼Œæœ€å¤š {max_pages} é¡µ...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        while page <= max_pages:\n",
    "            page_data = self._fetch_single_page(page)\n",
    "            \n",
    "            if not page_data:\n",
    "                print(f\"ç¬¬ {page} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢æŠ“å–\")\n",
    "                break\n",
    "            \n",
    "            all_questions.extend(page_data)\n",
    "            print(f\"ç¬¬ {page} é¡µ: æŠ“åˆ° {len(page_data)} ä¸ªé—®é¢˜ï¼Œç´¯è®¡ {len(all_questions)} ä¸ª\")\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ\n",
    "            if not self._has_next_page(page_data, page):\n",
    "                print(\"æ²¡æœ‰ä¸‹ä¸€é¡µäº†ï¼ŒæŠ“å–å®Œæˆ\")\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "            time.sleep(1.5)  # ç¤¼è²Œç­‰å¾…ï¼Œé¿å…ç»™æœåŠ¡å™¨å‹åŠ›\n",
    "        \n",
    "        print(f\"\\næŠ“å–å®Œæˆï¼å…±æŠ“å– {len(all_questions)} ä¸ªé—®é¢˜\")\n",
    "        return all_questions\n",
    "    \n",
    "    def _fetch_single_page(self, page_num: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        æŠ“å–å•ä¸ªé¡µé¢\n",
    "        \n",
    "        Args:\n",
    "            page_num: é¡µç \n",
    "            \n",
    "        Returns:\n",
    "            å½“å‰é¡µé—®é¢˜æ•°æ®åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æ„å»ºURL\n",
    "            if page_num == 1:\n",
    "                url = f\"{self.base_url}/questions\"\n",
    "            else:\n",
    "                url = f\"{self.base_url}/questions?page={page_num}\"\n",
    "            \n",
    "            print(f\"æŠ“å–ç¬¬ {page_num} é¡µ: {url}\")\n",
    "            \n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # ä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•\n",
    "            if page_num == 1:\n",
    "                self._save_page_content(response.text, 'first_page.html')\n",
    "            \n",
    "            # è§£æé¡µé¢\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # æŸ¥æ‰¾é—®é¢˜åˆ—è¡¨\n",
    "            question_items = soup.find_all('div', class_='list-group-item')\n",
    "            \n",
    "            if not question_items:\n",
    "                print(f\"è­¦å‘Š: ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°é—®é¢˜å…ƒç´ \")\n",
    "                return []\n",
    "            \n",
    "            # æå–é—®é¢˜æ•°æ®\n",
    "            page_questions = []\n",
    "            for item in question_items:\n",
    "                question_data = self._extract_question_data(item)\n",
    "                if question_data:\n",
    "                    question_data['æ¥æºé¡µç '] = page_num\n",
    "                    page_questions.append(question_data)\n",
    "            \n",
    "            return page_questions\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"ç¬¬ {page_num} é¡µè§£æå¤±è´¥: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_question_data(self, question_item) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        ä»å•ä¸ªé—®é¢˜å…ƒç´ ä¸­æå–æ•°æ®\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æå–é—®é¢˜æ ‡é¢˜å’Œé“¾æ¥\n",
    "            title_elem = question_item.find('a', class_='link-dark')\n",
    "            title = title_elem.text.strip() if title_elem else \"æœªçŸ¥æ ‡é¢˜\"\n",
    "            question_link = title_elem['href'] if title_elem else \"\"\n",
    "            \n",
    "            # æå–ç”¨æˆ·ä¿¡æ¯\n",
    "            user_elem = question_item.find('a', href=re.compile(r'/users/'))\n",
    "            user = user_elem.text.strip() if user_elem else \"åŒ¿åç”¨æˆ·\"\n",
    "            \n",
    "            # æå–å£°æœ›å€¼\n",
    "            reputation_elem = question_item.find('span', title='Reputation')\n",
    "            reputation = int(reputation_elem.text.strip()) if reputation_elem else 0\n",
    "            \n",
    "            # æå–æé—®æ—¶é—´\n",
    "            time_elem = question_item.find('time')\n",
    "            asked_time = time_elem.text.strip().replace('asked', '').strip() if time_elem else \"æœªçŸ¥æ—¶é—´\"\n",
    "            \n",
    "            # æå–ç²¾ç¡®æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰\n",
    "            datetime_str = \"\"\n",
    "            if time_elem and 'datetime' in time_elem.attrs:\n",
    "                datetime_str = time_elem['datetime']\n",
    "            \n",
    "            # æå–ç»Ÿè®¡ä¿¡æ¯\n",
    "            likes = self._extract_stat(question_item, 'bi-hand-thumbs-up-fill')\n",
    "            answers = self._extract_stat(question_item, 'bi-chat-square-text-fill')\n",
    "            views = self._extract_stat(question_item, 'bi-eye-fill')\n",
    "            \n",
    "            # æå–æ ‡ç­¾\n",
    "            tags = self._extract_tags(question_item)\n",
    "            \n",
    "            # æå–é—®é¢˜ID\n",
    "            question_id = self._extract_question_id(question_link)\n",
    "            \n",
    "            # æ„å»ºå®Œæ•´URL\n",
    "            full_question_url = f\"{self.base_url}{question_link}\" if question_link else \"\"\n",
    "            full_user_url = f\"{self.base_url}{user_elem['href']}\" if user_elem else \"\"\n",
    "            \n",
    "            return {\n",
    "                'é—®é¢˜ID': question_id,\n",
    "                'æ ‡é¢˜': title,\n",
    "                'æé—®ç”¨æˆ·': user,\n",
    "                'ç”¨æˆ·å£°æœ›': reputation,\n",
    "                'æé—®æ—¶é—´': asked_time,\n",
    "                'ç²¾ç¡®æ—¶é—´': datetime_str,\n",
    "                'ç‚¹èµæ•°': likes,\n",
    "                'å›ç­”æ•°': answers,\n",
    "                'æµè§ˆæ•°': views,\n",
    "                'æ ‡ç­¾': tags,\n",
    "                'é—®é¢˜é“¾æ¥': full_question_url,\n",
    "                'ç”¨æˆ·é“¾æ¥': full_user_url,\n",
    "                'æŠ“å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"æå–é—®é¢˜æ•°æ®æ—¶å‡ºé”™: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_stat(self, element, icon_class: str) -> int:\n",
    "        \"\"\"æå–ç»Ÿè®¡æ•°å­—\"\"\"\n",
    "        try:\n",
    "            elem = element.find('i', class_=icon_class)\n",
    "            if elem:\n",
    "                stat_elem = elem.find_next('em')\n",
    "                if stat_elem:\n",
    "                    return int(stat_elem.text.strip())\n",
    "        except:\n",
    "            pass\n",
    "        return 0\n",
    "    \n",
    "    def _extract_tags(self, element) -> str:\n",
    "        \"\"\"æå–æ ‡ç­¾\"\"\"\n",
    "        tags = []\n",
    "        try:\n",
    "            tag_elems = element.find_all('a', class_='badge-tag')\n",
    "            for tag_elem in tag_elems:\n",
    "                span = tag_elem.find('span')\n",
    "                if span:\n",
    "                    tags.append(span.text.strip())\n",
    "        except:\n",
    "            pass\n",
    "        return ', '.join(tags)\n",
    "    \n",
    "    def _extract_question_id(self, question_link: str) -> str:\n",
    "        \"\"\"æå–é—®é¢˜ID\"\"\"\n",
    "        try:\n",
    "            match = re.search(r'/questions/(\\d+)', question_link)\n",
    "            return match.group(1) if match else \"\"\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def _has_next_page(self, current_page_data: List, current_page: int) -> bool:\n",
    "        \"\"\"æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ\"\"\"\n",
    "        # å¦‚æœå½“å‰é¡µæ•°æ®å°‘äº10ä¸ªï¼Œå¯èƒ½æ²¡æœ‰ä¸‹ä¸€é¡µ\n",
    "        if len(current_page_data) < 10:\n",
    "            return False\n",
    "        \n",
    "        # å®é™…åº”è¯¥æ£€æŸ¥HTMLä¸­çš„åˆ†é¡µä¿¡æ¯ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†\n",
    "        # æœ€å¤šæŠ“å–20é¡µé˜²æ­¢æ— é™å¾ªç¯\n",
    "        return current_page < 20\n",
    "    \n",
    "    def _save_page_content(self, content: str, filename: str):\n",
    "        \"\"\"ä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "class DataAnalyzer:\n",
    "    \"\"\"æ•°æ®åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict]):\n",
    "        self.df = pd.DataFrame(data)\n",
    "        self.total_records = len(self.df)\n",
    "    \n",
    "    def analyze(self) -> Dict:\n",
    "        \"\"\"æ‰§è¡Œå…¨é¢åˆ†æ\"\"\"\n",
    "        if self.total_records == 0:\n",
    "            return {\"error\": \"æ²¡æœ‰æ•°æ®å¯åˆ†æ\"}\n",
    "        \n",
    "        analysis = {\n",
    "            \"basic_stats\": self._get_basic_stats(),\n",
    "            \"top_questions\": self._get_top_questions(),\n",
    "            \"user_analysis\": self._get_user_analysis(),\n",
    "            \"tag_analysis\": self._get_tag_analysis(),\n",
    "            \"time_analysis\": self._get_time_analysis(),\n",
    "            \"page_distribution\": self._get_page_distribution()\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _get_basic_stats(self) -> Dict:\n",
    "        \"\"\"è·å–åŸºç¡€ç»Ÿè®¡\"\"\"\n",
    "        stats = {\n",
    "            \"æ€»é—®é¢˜æ•°\": self.total_records,\n",
    "            \"æ€»æµè§ˆæ•°\": int(self.df['æµè§ˆæ•°'].sum()),\n",
    "            \"æ€»ç‚¹èµæ•°\": int(self.df['ç‚¹èµæ•°'].sum()),\n",
    "            \"æ€»å›ç­”æ•°\": int(self.df['å›ç­”æ•°'].sum()),\n",
    "            \"æ€»ç”¨æˆ·å£°æœ›\": int(self.df['ç”¨æˆ·å£°æœ›'].sum()),\n",
    "            \"å¹³å‡æµè§ˆæ•°\": float(self.df['æµè§ˆæ•°'].mean()),\n",
    "            \"å¹³å‡ç‚¹èµæ•°\": float(self.df['ç‚¹èµæ•°'].mean()),\n",
    "            \"å¹³å‡å›ç­”æ•°\": float(self.df['å›ç­”æ•°'].mean()),\n",
    "            \"æœ€å¤§æµè§ˆæ•°\": int(self.df['æµè§ˆæ•°'].max()),\n",
    "            \"æœ€å°æµè§ˆæ•°\": int(self.df['æµè§ˆæ•°'].min()),\n",
    "            \"é—®é¢˜IDèŒƒå›´\": f\"{self.df['é—®é¢˜ID'].min()} - {self.df['é—®é¢˜ID'].max()}\" if 'é—®é¢˜ID' in self.df.columns else \"N/A\"\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def _get_top_questions(self, n: int = 10) -> List[Dict]:\n",
    "        \"\"\"è·å–æœ€çƒ­é—¨çš„é—®é¢˜\"\"\"\n",
    "        if 'æµè§ˆæ•°' not in self.df.columns:\n",
    "            return []\n",
    "        \n",
    "        top_df = self.df.nlargest(n, 'æµè§ˆæ•°')[['æ ‡é¢˜', 'æµè§ˆæ•°', 'ç‚¹èµæ•°', 'å›ç­”æ•°', 'æé—®æ—¶é—´', 'é—®é¢˜é“¾æ¥']]\n",
    "        return top_df.to_dict('records')\n",
    "    \n",
    "    def _get_user_analysis(self) -> Dict:\n",
    "        \"\"\"ç”¨æˆ·åˆ†æ\"\"\"\n",
    "        if 'æé—®ç”¨æˆ·' not in self.df.columns:\n",
    "            return {}\n",
    "        \n",
    "        user_stats = self.df.groupby('æé—®ç”¨æˆ·').agg({\n",
    "            'æµè§ˆæ•°': 'sum',\n",
    "            'ç‚¹èµæ•°': 'sum',\n",
    "            'å›ç­”æ•°': 'sum',\n",
    "            'æ ‡é¢˜': 'count',\n",
    "            'ç”¨æˆ·å£°æœ›': 'max'\n",
    "        }).rename(columns={'æ ‡é¢˜': 'æé—®æ•°'}).sort_values('æé—®æ•°', ascending=False)\n",
    "        \n",
    "        top_users = user_stats.head(10).to_dict('index')\n",
    "        \n",
    "        return {\n",
    "            \"æ€»ç”¨æˆ·æ•°\": len(user_stats),\n",
    "            \"å¹³å‡æ¯ä¸ªç”¨æˆ·æé—®æ•°\": float(user_stats['æé—®æ•°'].mean()),\n",
    "            \"æœ€å¤šæé—®ç”¨æˆ·\": user_stats.index[0] if len(user_stats) > 0 else \"N/A\",\n",
    "            \"æœ€é«˜å£°æœ›ç”¨æˆ·\": self.df.loc[self.df['ç”¨æˆ·å£°æœ›'].idxmax(), 'æé—®ç”¨æˆ·'] if len(self.df) > 0 else \"N/A\",\n",
    "            \"å‰åæ´»è·ƒç”¨æˆ·\": top_users\n",
    "        }\n",
    "    \n",
    "    def _get_tag_analysis(self) -> Dict:\n",
    "        \"\"\"æ ‡ç­¾åˆ†æ\"\"\"\n",
    "        if 'æ ‡ç­¾' not in self.df.columns:\n",
    "            return {}\n",
    "        \n",
    "        # å±•å¼€æ‰€æœ‰æ ‡ç­¾\n",
    "        all_tags = []\n",
    "        for tags_str in self.df['æ ‡ç­¾']:\n",
    "            if tags_str and isinstance(tags_str, str):\n",
    "                tags = [tag.strip() for tag in tags_str.split(',')]\n",
    "                all_tags.extend(tags)\n",
    "        \n",
    "        if not all_tags:\n",
    "            return {}\n",
    "        \n",
    "        from collections import Counter\n",
    "        tag_counts = Counter(all_tags)\n",
    "        \n",
    "        return {\n",
    "            \"æ€»æ ‡ç­¾æ•°\": len(tag_counts),\n",
    "            \"æœ€å¸¸ç”¨æ ‡ç­¾\": tag_counts.most_common(10),\n",
    "            \"å¹³å‡æ¯ä¸ªé—®é¢˜æ ‡ç­¾æ•°\": len(all_tags) / self.total_records if self.total_records > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def _get_time_analysis(self) -> Dict:\n",
    "        \"\"\"æ—¶é—´åˆ†æ\"\"\"\n",
    "        if 'æé—®æ—¶é—´' not in self.df.columns:\n",
    "            return {}\n",
    "        \n",
    "        # æå–æœˆä»½\n",
    "        try:\n",
    "            self.df['æœˆä»½'] = self.df['æé—®æ—¶é—´'].apply(self._extract_month)\n",
    "            monthly_stats = self.df.groupby('æœˆä»½').agg({\n",
    "                'æµè§ˆæ•°': 'sum',\n",
    "                'ç‚¹èµæ•°': 'sum',\n",
    "                'æ ‡é¢˜': 'count'\n",
    "            }).rename(columns={'æ ‡é¢˜': 'æé—®æ•°'}).sort_index(ascending=False)\n",
    "            \n",
    "            return {\n",
    "                \"æœˆåº¦ç»Ÿè®¡\": monthly_stats.head(12).to_dict('index'),\n",
    "                \"æœ€æ´»è·ƒæœˆä»½\": monthly_stats['æé—®æ•°'].idxmax() if len(monthly_stats) > 0 else \"N/A\",\n",
    "                \"æ—¶é—´èŒƒå›´\": f\"{self.df['æé—®æ—¶é—´'].min()} - {self.df['æé—®æ—¶é—´'].max()}\"\n",
    "            }\n",
    "        except:\n",
    "            return {}\n",
    "    \n",
    "    def _get_page_distribution(self) -> Dict:\n",
    "        \"\"\"é¡µé¢åˆ†å¸ƒåˆ†æ\"\"\"\n",
    "        if 'æ¥æºé¡µç ' not in self.df.columns:\n",
    "            return {}\n",
    "        \n",
    "        page_stats = self.df['æ¥æºé¡µç '].value_counts().sort_index()\n",
    "        \n",
    "        return {\n",
    "            \"æ€»é¡µæ•°\": len(page_stats),\n",
    "            \"æ¯é¡µé—®é¢˜æ•°ç»Ÿè®¡\": page_stats.to_dict(),\n",
    "            \"å¹³å‡æ¯é¡µé—®é¢˜æ•°\": float(page_stats.mean())\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_month(time_str: str) -> str:\n",
    "        \"\"\"ä»æ—¶é—´å­—ç¬¦ä¸²æå–æœˆä»½\"\"\"\n",
    "        try:\n",
    "            # åŒ¹é…è‹±æ–‡æ ¼å¼: Dec 2, 2025\n",
    "            match = re.search(r'(\\w+)\\s+\\d+,\\s+(\\d{4})', time_str)\n",
    "            if match:\n",
    "                return f\"{match.group(1)} {match.group(2)}\"\n",
    "            \n",
    "            # åŒ¹é…ä¸­æ–‡æ ¼å¼: 2025å¹´12æœˆ31æ—¥\n",
    "            match = re.search(r'(\\d{4})å¹´(\\d{1,2})æœˆ', time_str)\n",
    "            if match:\n",
    "                months_en = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                            'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "                month_num = int(match.group(2))\n",
    "                if 1 <= month_num <= 12:\n",
    "                    return f\"{months_en[month_num-1]} {match.group(1)}\"\n",
    "            \n",
    "            return time_str[:7] if len(time_str) >= 7 else time_str\n",
    "        except:\n",
    "            return \"æœªçŸ¥æœˆä»½\"\n",
    "\n",
    "class DataExporter:\n",
    "    \"\"\"æ•°æ®å¯¼å‡ºå™¨\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_all(df: pd.DataFrame, base_filename: str):\n",
    "        \"\"\"å¯¼å‡ºæ‰€æœ‰æ ¼å¼çš„æ•°æ®\"\"\"\n",
    "        \n",
    "        # 1. å¯¼å‡ºCSV\n",
    "        csv_path = f\"{base_filename}.csv\"\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_path}\")\n",
    "        \n",
    "        # 2. å¯¼å‡ºExcel\n",
    "        try:\n",
    "            excel_path = f\"{base_filename}.xlsx\"\n",
    "            df.to_excel(excel_path, index=False)\n",
    "            print(f\"âœ… Excelæ–‡ä»¶å·²ä¿å­˜: {excel_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Excelä¿å­˜å¤±è´¥: {e}\")\n",
    "        \n",
    "        # 3. å¯¼å‡ºJSON\n",
    "        try:\n",
    "            json_path = f\"{base_filename}.json\"\n",
    "            records = df.to_dict('records')\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  JSONä¿å­˜å¤±è´¥: {e}\")\n",
    "        \n",
    "        # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š\n",
    "        report_path = f\"{base_filename}_report.txt\"\n",
    "        DataExporter._generate_report(df, report_path)\n",
    "        print(f\"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"csv\": csv_path,\n",
    "            \"excel\": excel_path if 'excel_path' in locals() else None,\n",
    "            \"json\": json_path if 'json_path' in locals() else None,\n",
    "            \"report\": report_path\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _generate_report(df: pd.DataFrame, report_path: str):\n",
    "        \"\"\"ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š\"\"\"\n",
    "        analyzer = DataAnalyzer(df.to_dict('records'))\n",
    "        analysis = analyzer.analyze()\n",
    "        \n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 70 + \"\\n\")\n",
    "            f.write(\"å¤©å·¥å¼€ç‰©é—®ç­”ç«™å®Œæ•´æ•°æ®åˆ†ææŠ¥å‘Š\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"æ•°æ®æ¥æº: https://answer.chancefoundation.org.cn/\\n\")\n",
    "            f.write(f\"æ€»æ•°æ®é‡: {len(df)} æ¡è®°å½•\\n\\n\")\n",
    "            \n",
    "            # åŸºç¡€ç»Ÿè®¡\n",
    "            if \"basic_stats\" in analysis:\n",
    "                f.write(\"ğŸ“Š åŸºç¡€ç»Ÿè®¡ä¿¡æ¯\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                stats = analysis[\"basic_stats\"]\n",
    "                for key, value in stats.items():\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # çƒ­é—¨é—®é¢˜\n",
    "            if \"top_questions\" in analysis and analysis[\"top_questions\"]:\n",
    "                f.write(\"ğŸ”¥ æœ€çƒ­é—¨é—®é¢˜ï¼ˆæŒ‰æµè§ˆæ•°ï¼‰\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                for i, question in enumerate(analysis[\"top_questions\"][:10], 1):\n",
    "                    f.write(f\"{i}. {question['æ ‡é¢˜'][:80]}...\\n\")\n",
    "                    f.write(f\"   æµè§ˆ: {question['æµè§ˆæ•°']}, ç‚¹èµ: {question['ç‚¹èµæ•°']}, å›ç­”: {question['å›ç­”æ•°']}\\n\")\n",
    "                    f.write(f\"   æ—¶é—´: {question['æé—®æ—¶é—´']}\\n\\n\")\n",
    "            \n",
    "            # ç”¨æˆ·åˆ†æ\n",
    "            if \"user_analysis\" in analysis:\n",
    "                f.write(\"ğŸ‘¥ ç”¨æˆ·åˆ†æ\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                user_info = analysis[\"user_analysis\"]\n",
    "                f.write(f\"æ€»ç”¨æˆ·æ•°: {user_info.get('æ€»ç”¨æˆ·æ•°', 'N/A')}\\n\")\n",
    "                f.write(f\"æœ€å¤šæé—®ç”¨æˆ·: {user_info.get('æœ€å¤šæé—®ç”¨æˆ·', 'N/A')}\\n\")\n",
    "                f.write(f\"æœ€é«˜å£°æœ›ç”¨æˆ·: {user_info.get('æœ€é«˜å£°æœ›ç”¨æˆ·', 'N/A')}\\n\\n\")\n",
    "            \n",
    "            # æ ‡ç­¾åˆ†æ\n",
    "            if \"tag_analysis\" in analysis and analysis[\"tag_analysis\"]:\n",
    "                f.write(\"ğŸ·ï¸ æ ‡ç­¾åˆ†æ\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\n\")\n",
    "                tag_info = analysis[\"tag_analysis\"]\n",
    "                f.write(f\"æ€»æ ‡ç­¾æ•°: {tag_info.get('æ€»æ ‡ç­¾æ•°', 'N/A')}\\n\")\n",
    "                if \"æœ€å¸¸ç”¨æ ‡ç­¾\" in tag_info:\n",
    "                    f.write(\"æœ€å¸¸ç”¨æ ‡ç­¾:\\n\")\n",
    "                    for tag, count in tag_info[\"æœ€å¸¸ç”¨æ ‡ç­¾\"]:\n",
    "                        f.write(f\"  {tag}: {count}æ¬¡\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # è¯¦ç»†æ•°æ®è¡¨\n",
    "            f.write(\"ğŸ“‹ è¯¦ç»†æ•°æ®ï¼ˆå‰50æ¡ï¼‰\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(df.head(50).to_string(index=False))\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"å¤©å·¥å¼€ç‰©é—®ç­”ç«™ - å®Œæ•´æ•°æ®æŠ“å–ä¸åˆ†æç³»ç»Ÿ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # é…ç½®\n",
    "    BASE_URL = \"https://answer.chancefoundation.org.cn\"\n",
    "    OUTPUT_BASE = r\"D:\\æ¡Œé¢\\opendiggerå¼€å‘è€…å¤§èµ›\\questions_data_full\"\n",
    "    MAX_PAGES = 20  # æœ€å¤§æŠ“å–é¡µæ•°ï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´\n",
    "    \n",
    "    print(f\"ç›®æ ‡ç½‘ç«™: {BASE_URL}\")\n",
    "    print(f\"è¾“å‡ºæ–‡ä»¶: {OUTPUT_BASE}.*\")\n",
    "    print(f\"æœ€å¤§é¡µæ•°: {MAX_PAGES}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # 1. åˆ›å»ºçˆ¬è™«å®ä¾‹\n",
    "    crawler = AnswerSiteCrawler(BASE_URL)\n",
    "    \n",
    "    # 2. æŠ“å–æ‰€æœ‰æ•°æ®\n",
    "    print(\"å¼€å§‹æŠ“å–æ•°æ®...\")\n",
    "    all_questions = crawler.fetch_all_questions(max_pages=MAX_PAGES)\n",
    "    \n",
    "    if not all_questions:\n",
    "        print(\"âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œç¨‹åºé€€å‡º\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nâœ… æŠ“å–å®Œæˆï¼å…±è·å¾— {len(all_questions)} æ¡æ•°æ®\")\n",
    "    \n",
    "    # 3. è½¬æ¢ä¸ºDataFrame\n",
    "    df = pd.DataFrame(all_questions)\n",
    "    \n",
    "    # 4. æ•°æ®åˆ†æ\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"æ•°æ®åˆ†æä¸­...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    analyzer = DataAnalyzer(all_questions)\n",
    "    analysis = analyzer.analyze()\n",
    "    \n",
    "    # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡\n",
    "    if \"basic_stats\" in analysis:\n",
    "        stats = analysis[\"basic_stats\"]\n",
    "        print(f\"ğŸ“Š å…³é”®ç»Ÿè®¡:\")\n",
    "        print(f\"  æ€»é—®é¢˜æ•°: {stats.get('æ€»é—®é¢˜æ•°', 'N/A')}\")\n",
    "        print(f\"  æ€»æµè§ˆæ•°: {stats.get('æ€»æµè§ˆæ•°', 'N/A'):,}\")\n",
    "        print(f\"  æ€»ç‚¹èµæ•°: {stats.get('æ€»ç‚¹èµæ•°', 'N/A'):,}\")\n",
    "        print(f\"  æ€»å›ç­”æ•°: {stats.get('æ€»å›ç­”æ•°', 'N/A'):,}\")\n",
    "        print(f\"  å¹³å‡æµè§ˆæ•°: {stats.get('å¹³å‡æµè§ˆæ•°', 'N/A'):.1f}\")\n",
    "    \n",
    "    if \"top_questions\" in analysis and analysis[\"top_questions\"]:\n",
    "        print(f\"\\nğŸ”¥ æœ€çƒ­é—¨é—®é¢˜:\")\n",
    "        for i, q in enumerate(analysis[\"top_questions\"][:3], 1):\n",
    "            print(f\"  {i}. {q['æ ‡é¢˜'][:60]}...\")\n",
    "            print(f\"     æµè§ˆ: {q['æµè§ˆæ•°']}æ¬¡, æ—¶é—´: {q['æé—®æ—¶é—´']}\")\n",
    "    \n",
    "    if \"user_analysis\" in analysis:\n",
    "        user_info = analysis[\"user_analysis\"]\n",
    "        print(f\"\\nğŸ‘¥ ç”¨æˆ·ç»Ÿè®¡:\")\n",
    "        print(f\"  æ€»ç”¨æˆ·æ•°: {user_info.get('æ€»ç”¨æˆ·æ•°', 'N/A')}\")\n",
    "        print(f\"  æœ€å¤šæé—®ç”¨æˆ·: {user_info.get('æœ€å¤šæé—®ç”¨æˆ·', 'N/A')}\")\n",
    "    \n",
    "    # 5. å¯¼å‡ºæ•°æ®\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"å¯¼å‡ºæ•°æ®...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    export_results = DataExporter.export_all(df, OUTPUT_BASE)\n",
    "    \n",
    "    # 6. æ•°æ®é¢„è§ˆ\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"æ•°æ®é¢„è§ˆ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦\n",
    "    print(f\"æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—\")\n",
    "    print(f\"\\nåˆ—å: {', '.join(df.columns.tolist())}\")\n",
    "    \n",
    "    print(f\"\\nå‰5æ¡æ•°æ®:\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 60)\n",
    "    print(df.head(5).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ğŸ‰ ä»»åŠ¡å®Œæˆï¼\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if export_results.get(\"csv\"):\n",
    "        print(f\"ğŸ“ ä¸»è¦æ•°æ®æ–‡ä»¶: {export_results['csv']}\")\n",
    "    if export_results.get(\"report\"):\n",
    "        print(f\"ğŸ“„ è¯¦ç»†æŠ¥å‘Šæ–‡ä»¶: {export_results['report']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ æç¤º:\")\n",
    "    print(f\"  - ä½¿ç”¨ Excel æ‰“å¼€ .xlsx æ–‡ä»¶è¿›è¡Œæ•°æ®åˆ†æ\")\n",
    "    print(f\"  - ä½¿ç”¨ æ–‡æœ¬ç¼–è¾‘å™¨ æ‰“å¼€ _report.txt æŸ¥çœ‹è¯¦ç»†åˆ†æ\")\n",
    "    print(f\"  - å…±æŠ“å– {len(df)} æ¡å®Œæ•´æ•°æ®\")\n",
    "\n",
    "def test_connection():\n",
    "    \"\"\"æµ‹è¯•è¿æ¥\"\"\"\n",
    "    print(\"æµ‹è¯•ç½‘ç«™è¿æ¥...\")\n",
    "    try:\n",
    "        response = requests.get(\"https://answer.chancefoundation.org.cn/questions\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ… ç½‘ç«™å¯æ­£å¸¸è®¿é—®\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ ç½‘ç«™è®¿é—®å¼‚å¸¸: HTTP {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¿æ¥å¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 120)\n",
    "    pd.set_option('display.max_colwidth', 80)\n",
    "    \n",
    "    # æµ‹è¯•è¿æ¥\n",
    "    if test_connection():\n",
    "        # è¿è¡Œä¸»ç¨‹åº\n",
    "        main()\n",
    "    else:\n",
    "        print(\"è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
