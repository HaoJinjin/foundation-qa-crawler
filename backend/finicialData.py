#!/usr/bin/env python
# coding: utf-8
import os
import sys

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
OUTPUT_DIR = os.path.join(BASE_DIR, 'backendData', 'output')
INPUT_DIR = os.path.join(BASE_DIR, 'backendData', 'input')

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(INPUT_DIR, exist_ok=True)

# In[6]:


#è§£æé¡µé¢ç»“æ„
import requests

url = "https://answer.chancefoundation.org.cn/"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

response = requests.get(url, headers=headers, timeout=10)

# ä¿å­˜åŸå§‹å“åº”
with open(os.path.join(OUTPUT_DIR, 'debug_output.txt'), 'w', encoding='utf-8') as f:
    f.write(response.text)

print(f"çŠ¶æ€ç : {response.status_code}")
print(f"å†…å®¹å¤§å°: {len(response.text)} å­—ç¬¦")

# æŸ¥æ‰¾å…³é”®ä¿¡æ¯
lines = response.text.split('\n')
print("\nåŒ…å«'å›ç­”äº'çš„è¡Œ:")
for i, line in enumerate(lines):
    if 'å›ç­”äº' in line:
        print(f"è¡Œ {i}: {line.strip()[:100]}")

print("\nè¯·å°† debug_output.txt æ–‡ä»¶çš„å‰1000å­—ç¬¦å‘ç»™æˆ‘")


# In[7]:


# å°è¯•çˆ¬å–ç¬¬ä¸€é¡µæ•°æ®
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time

def fetch_all_questions(base_url, max_pages=10):
    """
    æŠ“å–æ‰€æœ‰é¡µé¢é—®é¢˜æ•°æ®
    """
    all_questions = []
    page = 1
    
    while page <= max_pages:
        # æ„å»ºé¡µé¢URL
        if page == 1:
            url = base_url + "questions"
        else:
            url = f"{base_url}questions?page={page}"
        
        print(f"æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ: {url}")
        
        try:
            # å‘é€è¯·æ±‚
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code != 200:
                print(f"ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥: {response.status_code}")
                break
            
            # è§£æé¡µé¢
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾é—®é¢˜åˆ—è¡¨
            question_items = soup.find_all('div', class_='list-group-item')
            
            if not question_items:
                print(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°é—®é¢˜")
                break
            
            print(f"ç¬¬ {page} é¡µæ‰¾åˆ° {len(question_items)} ä¸ªé—®é¢˜")
            
            # æå–æ¯ä¸ªé—®é¢˜çš„æ•°æ®
            for item in question_items:
                question_data = extract_question_data(item)
                if question_data:
                    all_questions.append(question_data)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            next_page = soup.find('a', string='Next')
            if not next_page or page >= max_pages:
                break
            
            page += 1
            time.sleep(1)  # ç¤¼è²Œç­‰å¾…
            
        except Exception as e:
            print(f"æŠ“å–ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}")
            break
    
    return all_questions

def extract_question_data(question_item):
    """
    ä»å•ä¸ªé—®é¢˜å…ƒç´ ä¸­æå–æ•°æ®
    """
    try:
        # æå–é—®é¢˜æ ‡é¢˜
        title_elem = question_item.find('a', class_='link-dark')
        title = title_elem.text.strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
        
        # æå–é—®é¢˜é“¾æ¥
        question_link = title_elem['href'] if title_elem else ""
        
        # æå–ç”¨æˆ·ä¿¡æ¯
        user_elem = question_item.find('a', href=re.compile(r'/users/'))
        user = user_elem.text.strip() if user_elem else "åŒ¿åç”¨æˆ·"
        
        # æå–å£°æœ›å€¼
        reputation_elem = question_item.find('span', title='Reputation')
        reputation = int(reputation_elem.text.strip()) if reputation_elem else 0
        
        # æå–æé—®æ—¶é—´ - æŸ¥æ‰¾timeæ ‡ç­¾
        time_elem = question_item.find('time')
        asked_time = time_elem.text.strip().replace('asked', '').strip() if time_elem else "æœªçŸ¥æ—¶é—´"
        
        # æå–ç‚¹èµæ•° - æŸ¥æ‰¾åŒ…å«hand-thumbs-upçš„iæ ‡ç­¾
        likes_elem = question_item.find('i', class_='bi-hand-thumbs-up-fill')
        if likes_elem:
            likes = int(likes_elem.find_next('em').text.strip())
        else:
            likes = 0
        
        # æå–å›ç­”æ•°
        answers_elem = question_item.find('i', class_='bi-chat-square-text-fill')
        if answers_elem:
            answers = int(answers_elem.find_next('em').text.strip())
        else:
            answers = 0
        
        # æå–æµè§ˆæ•°
        views_elem = question_item.find('i', class_='bi-eye-fill')
        if views_elem:
            views = int(views_elem.find_next('em').text.strip())
        else:
            views = 0
        
        # æå–æ ‡ç­¾
        tags = []
        tag_elems = question_item.find_all('a', class_='badge-tag')
        for tag_elem in tag_elems:
            tag_text = tag_elem.find('span').text.strip()
            tags.append(tag_text)
        
        # æå–datetimeå±æ€§ä¸­çš„ç²¾ç¡®æ—¶é—´
        datetime_str = ""
        if time_elem and 'datetime' in time_elem.attrs:
            datetime_str = time_elem['datetime']
        
        # æå–é—®é¢˜ID
        question_id = ""
        if question_link:
            match = re.search(r'/questions/(\d+)', question_link)
            if match:
                question_id = match.group(1)
        
        return {
            'é—®é¢˜ID': question_id,
            'æ ‡é¢˜': title,
            'æé—®ç”¨æˆ·': user,
            'ç”¨æˆ·å£°æœ›': reputation,
            'æé—®æ—¶é—´': asked_time,
            'ç²¾ç¡®æ—¶é—´': datetime_str,
            'ç‚¹èµæ•°': likes,
            'å›ç­”æ•°': answers,
            'æµè§ˆæ•°': views,
            'æ ‡ç­¾': ', '.join(tags),
            'é—®é¢˜é“¾æ¥': f"https://answer.chancefoundation.org.cn{question_link}" if question_link else "",
            'ç”¨æˆ·é“¾æ¥': f"https://answer.chancefoundation.org.cn{user_elem['href']}" if user_elem else ""
        }
        
    except Exception as e:
        print(f"æå–é—®é¢˜æ•°æ®æ—¶å‡ºé”™: {e}")
        return None

def analyze_questions_data(questions):
    """
    åˆ†æé—®é¢˜æ•°æ®
    """
    if not questions:
        print("æ²¡æœ‰æ•°æ®å¯åˆ†æ")
        return None
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(questions)
    
    print(f"\n{'='*70}")
    print("æ•°æ®åˆ†ææŠ¥å‘Š")
    print(f"{'='*70}")
    
    print(f"âœ… æˆåŠŸæŠ“å– {len(df)} ä¸ªé—®é¢˜")
    
    # åŸºç¡€ç»Ÿè®¡
    print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
    print(f"  æ€»æµè§ˆæ•°: {df['æµè§ˆæ•°'].sum():,}")
    print(f"  æ€»ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].sum():,}")
    print(f"  æ€»å›ç­”æ•°: {df['å›ç­”æ•°'].sum():,}")
    print(f"  æ€»ç”¨æˆ·å£°æœ›: {df['ç”¨æˆ·å£°æœ›'].sum():,}")
    
    # å¹³å‡å€¼
    print(f"\nğŸ“ˆ å¹³å‡å€¼:")
    print(f"  å¹³å‡æµè§ˆæ•°: {df['æµè§ˆæ•°'].mean():.1f}")
    print(f"  å¹³å‡ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].mean():.1f}")
    print(f"  å¹³å‡å›ç­”æ•°: {df['å›ç­”æ•°'].mean():.1f}")
    
    # æœ€å—æ¬¢è¿çš„é—®é¢˜
    if len(df) > 0:
        top_views = df.nlargest(5, 'æµè§ˆæ•°')[['æ ‡é¢˜', 'æµè§ˆæ•°', 'ç‚¹èµæ•°', 'æé—®æ—¶é—´']]
        print(f"\nğŸ”¥ æœ€å—æ¬¢è¿çš„é—®é¢˜ï¼ˆæŒ‰æµè§ˆæ•°ï¼‰:")
        for i, (_, row) in enumerate(top_views.iterrows(), 1):
            print(f"  {i}. {row['æ ‡é¢˜'][:50]}...")
            print(f"     æµè§ˆ: {row['æµè§ˆæ•°']}æ¬¡, ç‚¹èµ: {row['ç‚¹èµæ•°']}ä¸ª, æ—¶é—´: {row['æé—®æ—¶é—´']}")
    
    # ç”¨æˆ·åˆ†æ
    if 'æé—®ç”¨æˆ·' in df.columns:
        user_stats = df.groupby('æé—®ç”¨æˆ·').agg({
            'æµè§ˆæ•°': 'sum',
            'ç‚¹èµæ•°': 'sum',
            'å›ç­”æ•°': 'sum',
            'æ ‡é¢˜': 'count'
        }).rename(columns={'æ ‡é¢˜': 'æé—®æ•°'}).sort_values('æé—®æ•°', ascending=False)
        
        print(f"\nğŸ‘¥ ç”¨æˆ·è´¡çŒ®ç»Ÿè®¡:")
        print(user_stats.head(10).to_string())
    
    # æ ‡ç­¾åˆ†æ
    if 'æ ‡ç­¾' in df.columns:
        # å±•å¼€æ ‡ç­¾æ•°æ®
        all_tags = []
        for tags_str in df['æ ‡ç­¾']:
            if tags_str:
                tags = [tag.strip() for tag in tags_str.split(',')]
                all_tags.extend(tags)
        
        if all_tags:
            from collections import Counter
            tag_counts = Counter(all_tags)
            
            print(f"\nğŸ·ï¸ çƒ­é—¨æ ‡ç­¾:")
            for tag, count in tag_counts.most_common(10):
                print(f"  {tag}: {count}æ¬¡")
    
    # æ—¶é—´åˆ†æï¼ˆå¦‚æœå¯èƒ½ï¼‰
    if 'æé—®æ—¶é—´' in df.columns:
        # æå–æœˆä»½ä¿¡æ¯
        try:
            df['æœˆä»½'] = df['æé—®æ—¶é—´'].apply(extract_month)
            monthly_stats = df.groupby('æœˆä»½').agg({
                'æµè§ˆæ•°': 'sum',
                'ç‚¹èµæ•°': 'sum',
                'æ ‡é¢˜': 'count'
            }).rename(columns={'æ ‡é¢˜': 'æé—®æ•°'}).sort_index(ascending=False)
            
            print(f"\nğŸ“… æœˆåº¦ç»Ÿè®¡:")
            print(monthly_stats.to_string())
        except:
            pass
    
    return df

def extract_month(time_str):
    """ä»æ—¶é—´å­—ç¬¦ä¸²ä¸­æå–æœˆä»½"""
    try:
        # åŒ¹é… "Dec 2, 2025" æˆ–ç±»ä¼¼æ ¼å¼
        match = re.search(r'(\w+)\s+\d+,\s+(\d{4})', time_str)
        if match:
            return f"{match.group(1)} {match.group(2)}"
        
        # åŒ¹é…ä¸­æ–‡æ ¼å¼
        match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', time_str)
        if match:
            months_en = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
            month_num = int(match.group(2))
            if 1 <= month_num <= 12:
                return f"{months_en[month_num-1]} {match.group(1)}"
        
        return time_str[:7]
    except:
        return "æœªçŸ¥æœˆä»½"

def save_data(df, output_path):
    """
    ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
    """
    if df is None or df.empty:
        print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return
    
    # ä¿å­˜ä¸ºCSV
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° CSV: {output_path}")
    
    # ä¿å­˜ä¸ºExcel
    try:
        excel_path = output_path.replace('.csv', '.xlsx')
        df.to_excel(excel_path, index=False)
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° Excel: {excel_path}")
    except Exception as e:
        print(f"Excelä¿å­˜å¤±è´¥: {e}")
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_path = output_path.replace('.csv', '_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("å¤©å·¥å¼€ç‰©é—®ç­”ç«™æ•°æ®åˆ†ææŠ¥å‘Š\n")
        f.write("="*60 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\n")
        f.write(f"æ•°æ®æ¥æº: https://answer.chancefoundation.org.cn/\n")
        f.write(f"æ€»é—®é¢˜æ•°: {len(df)}\n")
        f.write(f"æ€»æµè§ˆæ•°: {df['æµè§ˆæ•°'].sum()}\n")
        f.write(f"æ€»ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].sum()}\n")
        f.write(f"æ€»å›ç­”æ•°: {df['å›ç­”æ•°'].sum()}\n")
        
        # çƒ­é—¨é—®é¢˜
        f.write("\nçƒ­é—¨é—®é¢˜:\n")
        top_5 = df.nlargest(5, 'æµè§ˆæ•°')
        for i, (_, row) in enumerate(top_5.iterrows(), 1):
            f.write(f"{i}. {row['æ ‡é¢˜']}\n")
            f.write(f"   æµè§ˆ: {row['æµè§ˆæ•°']}, ç‚¹èµ: {row['ç‚¹èµæ•°']}, å›ç­”: {row['å›ç­”æ•°']}, æ—¶é—´: {row['æé—®æ—¶é—´']}\n")
        
        # è¯¦ç»†æ•°æ®
        f.write("\nè¯¦ç»†æ•°æ®:\n")
        f.write(df.to_string(index=False))
    
    print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print(f"{'='*70}")
    print("å¤©å·¥å¼€ç‰©é—®ç­”ç«™æ•°æ®æŠ“å–å·¥å…·")
    print(f"{'='*70}")
    
    base_url = "https://answer.chancefoundation.org.cn/"
    output_path = os.path.join(OUTPUT_DIR, 'questions_data.csv')
    
    print(f"ç›®æ ‡ç½‘ç«™: {base_url}")
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")
    
    # æŠ“å–æ•°æ®
    print("\nå¼€å§‹æŠ“å–æ•°æ®...")
    questions = fetch_all_questions(base_url, max_pages=4)
    
    if questions:
        # åˆ†ææ•°æ®
        df = analyze_questions_data(questions)
        
        if df is not None and not df.empty:
            # ä¿å­˜æ•°æ®
            save_data(df, output_path)
            
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            print(f"\nğŸ“‹ æ•°æ®é¢„è§ˆï¼ˆå‰10æ¡ï¼‰:")
            pd.set_option('display.max_columns', None)
            pd.set_option('display.width', None)
            pd.set_option('display.max_colwidth', 50)
            print(df.head(10).to_string(index=False))
            
            print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼")
            print(f"   å…±æŠ“å– {len(df)} ä¸ªé—®é¢˜")
            print(f"   æ•°æ®å·²ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„")
        else:
            print("\nâŒ æ•°æ®æå–å¤±è´¥")
    else:
        print("\nâŒ æ²¡æœ‰æŠ“åˆ°ä»»ä½•æ•°æ®")

def quick_test():
    """
    å¿«é€Ÿæµ‹è¯•å‡½æ•° - æŠ“å–ç¬¬ä¸€é¡µæ•°æ®
    """
    print("å¿«é€Ÿæµ‹è¯• - æŠ“å–ç¬¬ä¸€é¡µæ•°æ®...")
    
    url = "https://answer.chancefoundation.org.cn/questions"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾é—®é¢˜åˆ—è¡¨
            question_items = soup.find_all('div', class_='list-group-item')
            print(f"æ‰¾åˆ° {len(question_items)} ä¸ªé—®é¢˜")
            
            # æå–å‰3ä¸ªé—®é¢˜ä½œä¸ºç¤ºä¾‹
            for i, item in enumerate(question_items[:3], 1):
                data = extract_question_data(item)
                if data:
                    print(f"\né—®é¢˜ {i}:")
                    print(f"  æ ‡é¢˜: {data['æ ‡é¢˜'][:50]}...")
                    print(f"  ç”¨æˆ·: {data['æé—®ç”¨æˆ·']}")
                    print(f"  æ—¶é—´: {data['æé—®æ—¶é—´']}")
                    print(f"  æµè§ˆ: {data['æµè§ˆæ•°']}, ç‚¹èµ: {data['ç‚¹èµæ•°']}, å›ç­”: {data['å›ç­”æ•°']}")
            
            return True
        else:
            print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
            
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
    
    return False

if __name__ == "__main__":
    # è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 50)
    
    # é¦–å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•
    print("è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
    if quick_test():
        print("\nå¿«é€Ÿæµ‹è¯•æˆåŠŸï¼Œå¼€å§‹å®Œæ•´æŠ“å–...")
        main()
    else:
        print("\nå¿«é€Ÿæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç½‘ç«™è®¿é—®")


# In[8]:


#å°è¯•çˆ¬å–æ‰€æœ‰é—®ç­”æ•°æ®
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import json
from datetime import datetime
import os
from typing import List, Dict, Optional

class AnswerSiteCrawler:
    """å¤©å·¥å¼€ç‰©é—®ç­”ç«™å®Œæ•´æ•°æ®æŠ“å–å™¨"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
    def fetch_all_questions(self, max_pages: int = 20) -> List[Dict]:
        """
        æŠ“å–æ‰€æœ‰é¡µé¢é—®é¢˜æ•°æ®
        
        Args:
            max_pages: æœ€å¤§æŠ“å–é¡µæ•°
            
        Returns:
            é—®é¢˜æ•°æ®åˆ—è¡¨
        """
        all_questions = []
        page = 1
        
        print(f"å¼€å§‹æŠ“å–æ•°æ®ï¼Œæœ€å¤š {max_pages} é¡µ...")
        print("-" * 60)
        
        while page <= max_pages:
            page_data = self._fetch_single_page(page)
            
            if not page_data:
                print(f"ç¬¬ {page} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢æŠ“å–")
                break
            
            all_questions.extend(page_data)
            print(f"ç¬¬ {page} é¡µ: æŠ“åˆ° {len(page_data)} ä¸ªé—®é¢˜ï¼Œç´¯è®¡ {len(all_questions)} ä¸ª")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            if not self._has_next_page(page_data, page):
                print("æ²¡æœ‰ä¸‹ä¸€é¡µäº†ï¼ŒæŠ“å–å®Œæˆ")
                break
            
            page += 1
            time.sleep(1.5)  # ç¤¼è²Œç­‰å¾…ï¼Œé¿å…ç»™æœåŠ¡å™¨å‹åŠ›
        
        print(f"\næŠ“å–å®Œæˆï¼å…±æŠ“å– {len(all_questions)} ä¸ªé—®é¢˜")
        return all_questions
    
    def _fetch_single_page(self, page_num: int) -> List[Dict]:
        """
        æŠ“å–å•ä¸ªé¡µé¢
        
        Args:
            page_num: é¡µç 
            
        Returns:
            å½“å‰é¡µé—®é¢˜æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºURL
            if page_num == 1:
                url = f"{self.base_url}/questions"
            else:
                url = f"{self.base_url}/questions?page={page_num}"
            
            print(f"æŠ“å–ç¬¬ {page_num} é¡µ: {url}")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # ä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•
            if page_num == 1:
                self._save_page_content(response.text, 'first_page.html')
            
            # è§£æé¡µé¢
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾é—®é¢˜åˆ—è¡¨
            question_items = soup.find_all('div', class_='list-group-item')
            
            if not question_items:
                print(f"è­¦å‘Š: ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°é—®é¢˜å…ƒç´ ")
                return []
            
            # æå–é—®é¢˜æ•°æ®
            page_questions = []
            for item in question_items:
                question_data = self._extract_question_data(item)
                if question_data:
                    question_data['æ¥æºé¡µç '] = page_num
                    page_questions.append(question_data)
            
            return page_questions
            
        except requests.RequestException as e:
            print(f"ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"ç¬¬ {page_num} é¡µè§£æå¤±è´¥: {e}")
            return []
    
    def _extract_question_data(self, question_item) -> Optional[Dict]:
        """
        ä»å•ä¸ªé—®é¢˜å…ƒç´ ä¸­æå–æ•°æ®
        """
        try:
            # æå–é—®é¢˜æ ‡é¢˜å’Œé“¾æ¥
            title_elem = question_item.find('a', class_='link-dark')
            title = title_elem.text.strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
            question_link = title_elem['href'] if title_elem else ""
            
            # æå–ç”¨æˆ·ä¿¡æ¯
            user_elem = question_item.find('a', href=re.compile(r'/users/'))
            user = user_elem.text.strip() if user_elem else "åŒ¿åç”¨æˆ·"
            
            # æå–å£°æœ›å€¼
            reputation_elem = question_item.find('span', title='Reputation')
            reputation = int(reputation_elem.text.strip()) if reputation_elem else 0
            
            # æå–æé—®æ—¶é—´
            time_elem = question_item.find('time')
            asked_time = time_elem.text.strip().replace('asked', '').strip() if time_elem else "æœªçŸ¥æ—¶é—´"
            
            # æå–ç²¾ç¡®æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
            datetime_str = ""
            if time_elem and 'datetime' in time_elem.attrs:
                datetime_str = time_elem['datetime']
            
            # æå–ç»Ÿè®¡ä¿¡æ¯
            likes = self._extract_stat(question_item, 'bi-hand-thumbs-up-fill')
            answers = self._extract_stat(question_item, 'bi-chat-square-text-fill')
            views = self._extract_stat(question_item, 'bi-eye-fill')
            
            # æå–æ ‡ç­¾
            tags = self._extract_tags(question_item)
            
            # æå–é—®é¢˜ID
            question_id = self._extract_question_id(question_link)
            
            # æ„å»ºå®Œæ•´URL
            full_question_url = f"{self.base_url}{question_link}" if question_link else ""
            full_user_url = f"{self.base_url}{user_elem['href']}" if user_elem else ""
            
            return {
                'é—®é¢˜ID': question_id,
                'æ ‡é¢˜': title,
                'æé—®ç”¨æˆ·': user,
                'ç”¨æˆ·å£°æœ›': reputation,
                'æé—®æ—¶é—´': asked_time,
                'ç²¾ç¡®æ—¶é—´': datetime_str,
                'ç‚¹èµæ•°': likes,
                'å›ç­”æ•°': answers,
                'æµè§ˆæ•°': views,
                'æ ‡ç­¾': tags,
                'é—®é¢˜é“¾æ¥': full_question_url,
                'ç”¨æˆ·é“¾æ¥': full_user_url,
                'æŠ“å–æ—¶é—´': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            print(f"æå–é—®é¢˜æ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    def _extract_stat(self, element, icon_class: str) -> int:
        """æå–ç»Ÿè®¡æ•°å­—"""
        try:
            elem = element.find('i', class_=icon_class)
            if elem:
                stat_elem = elem.find_next('em')
                if stat_elem:
                    return int(stat_elem.text.strip())
        except:
            pass
        return 0
    
    def _extract_tags(self, element) -> str:
        """æå–æ ‡ç­¾"""
        tags = []
        try:
            tag_elems = element.find_all('a', class_='badge-tag')
            for tag_elem in tag_elems:
                span = tag_elem.find('span')
                if span:
                    tags.append(span.text.strip())
        except:
            pass
        return ', '.join(tags)
    
    def _extract_question_id(self, question_link: str) -> str:
        """æå–é—®é¢˜ID"""
        try:
            match = re.search(r'/questions/(\d+)', question_link)
            return match.group(1) if match else ""
        except:
            return ""
    
    def _has_next_page(self, current_page_data: List, current_page: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ"""
        # å¦‚æœå½“å‰é¡µæ•°æ®å°‘äº10ä¸ªï¼Œå¯èƒ½æ²¡æœ‰ä¸‹ä¸€é¡µ
        if len(current_page_data) < 10:
            return False
        
        # å®é™…åº”è¯¥æ£€æŸ¥HTMLä¸­çš„åˆ†é¡µä¿¡æ¯ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        # æœ€å¤šæŠ“å–20é¡µé˜²æ­¢æ— é™å¾ªç¯
        return current_page < 20
    
    def _save_page_content(self, content: str, filename: str):
        """ä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•"""
        try:
            filepath = os.path.join(OUTPUT_DIR, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
        except:
            pass

class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨"""
    
    def __init__(self, data: List[Dict]):
        self.df = pd.DataFrame(data)
        self.total_records = len(self.df)
    
    def analyze(self) -> Dict:
        """æ‰§è¡Œå…¨é¢åˆ†æ"""
        if self.total_records == 0:
            return {"error": "æ²¡æœ‰æ•°æ®å¯åˆ†æ"}
        
        analysis = {
            "basic_stats": self._get_basic_stats(),
            "top_questions": self._get_top_questions(),
            "user_analysis": self._get_user_analysis(),
            "tag_analysis": self._get_tag_analysis(),
            "time_analysis": self._get_time_analysis(),
            "page_distribution": self._get_page_distribution()
        }
        
        return analysis
    
    def _get_basic_stats(self) -> Dict:
        """è·å–åŸºç¡€ç»Ÿè®¡"""
        stats = {
            "æ€»é—®é¢˜æ•°": self.total_records,
            "æ€»æµè§ˆæ•°": int(self.df['æµè§ˆæ•°'].sum()),
            "æ€»ç‚¹èµæ•°": int(self.df['ç‚¹èµæ•°'].sum()),
            "æ€»å›ç­”æ•°": int(self.df['å›ç­”æ•°'].sum()),
            "æ€»ç”¨æˆ·å£°æœ›": int(self.df['ç”¨æˆ·å£°æœ›'].sum()),
            "å¹³å‡æµè§ˆæ•°": float(self.df['æµè§ˆæ•°'].mean()),
            "å¹³å‡ç‚¹èµæ•°": float(self.df['ç‚¹èµæ•°'].mean()),
            "å¹³å‡å›ç­”æ•°": float(self.df['å›ç­”æ•°'].mean()),
            "æœ€å¤§æµè§ˆæ•°": int(self.df['æµè§ˆæ•°'].max()),
            "æœ€å°æµè§ˆæ•°": int(self.df['æµè§ˆæ•°'].min()),
            "é—®é¢˜IDèŒƒå›´": f"{self.df['é—®é¢˜ID'].min()} - {self.df['é—®é¢˜ID'].max()}" if 'é—®é¢˜ID' in self.df.columns else "N/A"
        }
        return stats
    
    def _get_top_questions(self, n: int = 10) -> List[Dict]:
        """è·å–æœ€çƒ­é—¨çš„é—®é¢˜"""
        if 'æµè§ˆæ•°' not in self.df.columns:
            return []
        
        top_df = self.df.nlargest(n, 'æµè§ˆæ•°')[['æ ‡é¢˜', 'æµè§ˆæ•°', 'ç‚¹èµæ•°', 'å›ç­”æ•°', 'æé—®æ—¶é—´', 'é—®é¢˜é“¾æ¥']]
        return top_df.to_dict('records')
    
    def _get_user_analysis(self) -> Dict:
        """ç”¨æˆ·åˆ†æ"""
        if 'æé—®ç”¨æˆ·' not in self.df.columns:
            return {}
        
        user_stats = self.df.groupby('æé—®ç”¨æˆ·').agg({
            'æµè§ˆæ•°': 'sum',
            'ç‚¹èµæ•°': 'sum',
            'å›ç­”æ•°': 'sum',
            'æ ‡é¢˜': 'count',
            'ç”¨æˆ·å£°æœ›': 'max'
        }).rename(columns={'æ ‡é¢˜': 'æé—®æ•°'}).sort_values('æé—®æ•°', ascending=False)
        
        top_users = user_stats.head(10).to_dict('index')
        
        return {
            "æ€»ç”¨æˆ·æ•°": len(user_stats),
            "å¹³å‡æ¯ä¸ªç”¨æˆ·æé—®æ•°": float(user_stats['æé—®æ•°'].mean()),
            "æœ€å¤šæé—®ç”¨æˆ·": user_stats.index[0] if len(user_stats) > 0 else "N/A",
            "æœ€é«˜å£°æœ›ç”¨æˆ·": self.df.loc[self.df['ç”¨æˆ·å£°æœ›'].idxmax(), 'æé—®ç”¨æˆ·'] if len(self.df) > 0 else "N/A",
            "å‰åæ´»è·ƒç”¨æˆ·": top_users
        }
    
    def _get_tag_analysis(self) -> Dict:
        """æ ‡ç­¾åˆ†æ"""
        if 'æ ‡ç­¾' not in self.df.columns:
            return {}
        
        # å±•å¼€æ‰€æœ‰æ ‡ç­¾
        all_tags = []
        for tags_str in self.df['æ ‡ç­¾']:
            if tags_str and isinstance(tags_str, str):
                tags = [tag.strip() for tag in tags_str.split(',')]
                all_tags.extend(tags)
        
        if not all_tags:
            return {}
        
        from collections import Counter
        tag_counts = Counter(all_tags)
        
        return {
            "æ€»æ ‡ç­¾æ•°": len(tag_counts),
            "æœ€å¸¸ç”¨æ ‡ç­¾": tag_counts.most_common(10),
            "å¹³å‡æ¯ä¸ªé—®é¢˜æ ‡ç­¾æ•°": len(all_tags) / self.total_records if self.total_records > 0 else 0
        }
    
    def _get_time_analysis(self) -> Dict:
        """æ—¶é—´åˆ†æ"""
        if 'æé—®æ—¶é—´' not in self.df.columns:
            return {}
        
        # æå–æœˆä»½
        try:
            self.df['æœˆä»½'] = self.df['æé—®æ—¶é—´'].apply(self._extract_month)
            monthly_stats = self.df.groupby('æœˆä»½').agg({
                'æµè§ˆæ•°': 'sum',
                'ç‚¹èµæ•°': 'sum',
                'æ ‡é¢˜': 'count'
            }).rename(columns={'æ ‡é¢˜': 'æé—®æ•°'}).sort_index(ascending=False)
            
            return {
                "æœˆåº¦ç»Ÿè®¡": monthly_stats.head(12).to_dict('index'),
                "æœ€æ´»è·ƒæœˆä»½": monthly_stats['æé—®æ•°'].idxmax() if len(monthly_stats) > 0 else "N/A",
                "æ—¶é—´èŒƒå›´": f"{self.df['æé—®æ—¶é—´'].min()} - {self.df['æé—®æ—¶é—´'].max()}"
            }
        except:
            return {}
    
    def _get_page_distribution(self) -> Dict:
        """é¡µé¢åˆ†å¸ƒåˆ†æ"""
        if 'æ¥æºé¡µç ' not in self.df.columns:
            return {}
        
        page_stats = self.df['æ¥æºé¡µç '].value_counts().sort_index()
        
        return {
            "æ€»é¡µæ•°": len(page_stats),
            "æ¯é¡µé—®é¢˜æ•°ç»Ÿè®¡": page_stats.to_dict(),
            "å¹³å‡æ¯é¡µé—®é¢˜æ•°": float(page_stats.mean())
        }
    
    @staticmethod
    def _extract_month(time_str: str) -> str:
        """ä»æ—¶é—´å­—ç¬¦ä¸²æå–æœˆä»½"""
        try:
            # åŒ¹é…è‹±æ–‡æ ¼å¼: Dec 2, 2025
            match = re.search(r'(\w+)\s+\d+,\s+(\d{4})', time_str)
            if match:
                return f"{match.group(1)} {match.group(2)}"
            
            # åŒ¹é…ä¸­æ–‡æ ¼å¼: 2025å¹´12æœˆ31æ—¥
            match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', time_str)
            if match:
                months_en = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                            'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
                month_num = int(match.group(2))
                if 1 <= month_num <= 12:
                    return f"{months_en[month_num-1]} {match.group(1)}"
            
            return time_str[:7] if len(time_str) >= 7 else time_str
        except:
            return "æœªçŸ¥æœˆä»½"

class DataExporter:
    """æ•°æ®å¯¼å‡ºå™¨"""
    
    @staticmethod
    def export_all(df: pd.DataFrame, base_filename: str):
        """å¯¼å‡ºæ‰€æœ‰æ ¼å¼çš„æ•°æ®"""
        
        # 1. å¯¼å‡ºCSV
        csv_path = f"{base_filename}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_path}")
        
        # 2. å¯¼å‡ºExcel
        try:
            excel_path = f"{base_filename}.xlsx"
            df.to_excel(excel_path, index=False)
            print(f"âœ… Excelæ–‡ä»¶å·²ä¿å­˜: {excel_path}")
        except Exception as e:
            print(f"âš ï¸  Excelä¿å­˜å¤±è´¥: {e}")
        
        # 3. å¯¼å‡ºJSON
        try:
            json_path = f"{base_filename}.json"
            records = df.to_dict('records')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(records, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_path}")
        except Exception as e:
            print(f"âš ï¸  JSONä¿å­˜å¤±è´¥: {e}")
        
        # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_path = f"{base_filename}_report.txt"
        DataExporter._generate_report(df, report_path)
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return {
            "csv": csv_path,
            "excel": excel_path if 'excel_path' in locals() else None,
            "json": json_path if 'json_path' in locals() else None,
            "report": report_path
        }
    
    @staticmethod
    def _generate_report(df: pd.DataFrame, report_path: str):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        analyzer = DataAnalyzer(df.to_dict('records'))
        analysis = analyzer.analyze()
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("å¤©å·¥å¼€ç‰©é—®ç­”ç«™å®Œæ•´æ•°æ®åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 70 + "\n\n")
            
            f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®æ¥æº: https://answer.chancefoundation.org.cn/\n")
            f.write(f"æ€»æ•°æ®é‡: {len(df)} æ¡è®°å½•\n\n")
            
            # åŸºç¡€ç»Ÿè®¡
            if "basic_stats" in analysis:
                f.write("ğŸ“Š åŸºç¡€ç»Ÿè®¡ä¿¡æ¯\n")
                f.write("-" * 40 + "\n")
                stats = analysis["basic_stats"]
                for key, value in stats.items():
                    f.write(f"{key}: {value}\n")
                f.write("\n")
            
            # çƒ­é—¨é—®é¢˜
            if "top_questions" in analysis and analysis["top_questions"]:
                f.write("ğŸ”¥ æœ€çƒ­é—¨é—®é¢˜ï¼ˆæŒ‰æµè§ˆæ•°ï¼‰\n")
                f.write("-" * 40 + "\n")
                for i, question in enumerate(analysis["top_questions"][:10], 1):
                    f.write(f"{i}. {question['æ ‡é¢˜'][:80]}...\n")
                    f.write(f"   æµè§ˆ: {question['æµè§ˆæ•°']}, ç‚¹èµ: {question['ç‚¹èµæ•°']}, å›ç­”: {question['å›ç­”æ•°']}\n")
                    f.write(f"   æ—¶é—´: {question['æé—®æ—¶é—´']}\n\n")
            
            # ç”¨æˆ·åˆ†æ
            if "user_analysis" in analysis:
                f.write("ğŸ‘¥ ç”¨æˆ·åˆ†æ\n")
                f.write("-" * 40 + "\n")
                user_info = analysis["user_analysis"]
                f.write(f"æ€»ç”¨æˆ·æ•°: {user_info.get('æ€»ç”¨æˆ·æ•°', 'N/A')}\n")
                f.write(f"æœ€å¤šæé—®ç”¨æˆ·: {user_info.get('æœ€å¤šæé—®ç”¨æˆ·', 'N/A')}\n")
                f.write(f"æœ€é«˜å£°æœ›ç”¨æˆ·: {user_info.get('æœ€é«˜å£°æœ›ç”¨æˆ·', 'N/A')}\n\n")
            
            # æ ‡ç­¾åˆ†æ
            if "tag_analysis" in analysis and analysis["tag_analysis"]:
                f.write("ğŸ·ï¸ æ ‡ç­¾åˆ†æ\n")
                f.write("-" * 40 + "\n")
                tag_info = analysis["tag_analysis"]
                f.write(f"æ€»æ ‡ç­¾æ•°: {tag_info.get('æ€»æ ‡ç­¾æ•°', 'N/A')}\n")
                if "æœ€å¸¸ç”¨æ ‡ç­¾" in tag_info:
                    f.write("æœ€å¸¸ç”¨æ ‡ç­¾:\n")
                    for tag, count in tag_info["æœ€å¸¸ç”¨æ ‡ç­¾"]:
                        f.write(f"  {tag}: {count}æ¬¡\n")
                f.write("\n")
            
            # è¯¦ç»†æ•°æ®è¡¨
            f.write("ğŸ“‹ è¯¦ç»†æ•°æ®ï¼ˆå‰50æ¡ï¼‰\n")
            f.write("-" * 40 + "\n")
            f.write(df.head(50).to_string(index=False))

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("å¤©å·¥å¼€ç‰©é—®ç­”ç«™ - å®Œæ•´æ•°æ®æŠ“å–ä¸åˆ†æç³»ç»Ÿ")
    print("=" * 70)
    
    # é…ç½®
    BASE_URL = "https://answer.chancefoundation.org.cn"
    OUTPUT_BASE = os.path.join(OUTPUT_DIR, 'questions_data_full')
    MAX_PAGES = 20  # æœ€å¤§æŠ“å–é¡µæ•°ï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    
    print(f"ç›®æ ‡ç½‘ç«™: {BASE_URL}")
    print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_BASE}.*")
    print(f"æœ€å¤§é¡µæ•°: {MAX_PAGES}")
    print("\n" + "=" * 70)
    
    # 1. åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AnswerSiteCrawler(BASE_URL)
    
    # 2. æŠ“å–æ‰€æœ‰æ•°æ®
    print("å¼€å§‹æŠ“å–æ•°æ®...")
    all_questions = crawler.fetch_all_questions(max_pages=MAX_PAGES)
    
    if not all_questions:
        print("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"\nâœ… æŠ“å–å®Œæˆï¼å…±è·å¾— {len(all_questions)} æ¡æ•°æ®")
    
    # 3. è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_questions)
    
    # 4. æ•°æ®åˆ†æ
    print("\n" + "=" * 70)
    print("æ•°æ®åˆ†æä¸­...")
    print("=" * 70)
    
    analyzer = DataAnalyzer(all_questions)
    analysis = analyzer.analyze()
    
    # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
    if "basic_stats" in analysis:
        stats = analysis["basic_stats"]
        print(f"ğŸ“Š å…³é”®ç»Ÿè®¡:")
        print(f"  æ€»é—®é¢˜æ•°: {stats.get('æ€»é—®é¢˜æ•°', 'N/A')}")
        print(f"  æ€»æµè§ˆæ•°: {stats.get('æ€»æµè§ˆæ•°', 'N/A'):,}")
        print(f"  æ€»ç‚¹èµæ•°: {stats.get('æ€»ç‚¹èµæ•°', 'N/A'):,}")
        print(f"  æ€»å›ç­”æ•°: {stats.get('æ€»å›ç­”æ•°', 'N/A'):,}")
        print(f"  å¹³å‡æµè§ˆæ•°: {stats.get('å¹³å‡æµè§ˆæ•°', 'N/A'):.1f}")
    
    if "top_questions" in analysis and analysis["top_questions"]:
        print(f"\nğŸ”¥ æœ€çƒ­é—¨é—®é¢˜:")
        for i, q in enumerate(analysis["top_questions"][:3], 1):
            print(f"  {i}. {q['æ ‡é¢˜'][:60]}...")
            print(f"     æµè§ˆ: {q['æµè§ˆæ•°']}æ¬¡, æ—¶é—´: {q['æé—®æ—¶é—´']}")
    
    if "user_analysis" in analysis:
        user_info = analysis["user_analysis"]
        print(f"\nğŸ‘¥ ç”¨æˆ·ç»Ÿè®¡:")
        print(f"  æ€»ç”¨æˆ·æ•°: {user_info.get('æ€»ç”¨æˆ·æ•°', 'N/A')}")
        print(f"  æœ€å¤šæé—®ç”¨æˆ·: {user_info.get('æœ€å¤šæé—®ç”¨æˆ·', 'N/A')}")
    
    # 5. å¯¼å‡ºæ•°æ®
    print("\n" + "=" * 70)
    print("å¯¼å‡ºæ•°æ®...")
    print("=" * 70)
    
    export_results = DataExporter.export_all(df, OUTPUT_BASE)
    
    # 6. æ•°æ®é¢„è§ˆ
    print("\n" + "=" * 70)
    print("æ•°æ®é¢„è§ˆ")
    print("=" * 70)
    
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    print(f"æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"\nåˆ—å: {', '.join(df.columns.tolist())}")
    
    print(f"\nå‰5æ¡æ•°æ®:")
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 60)
    print(df.head(5).to_string(index=False))
    
    print(f"\n{'='*70}")
    print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    print(f"{'='*70}")
    
    if export_results.get("csv"):
        print(f"ğŸ“ ä¸»è¦æ•°æ®æ–‡ä»¶: {export_results['csv']}")
    if export_results.get("report"):
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šæ–‡ä»¶: {export_results['report']}")
    
    print(f"\nğŸ’¡ æç¤º:")
    print(f"  - ä½¿ç”¨ Excel æ‰“å¼€ .xlsx æ–‡ä»¶è¿›è¡Œæ•°æ®åˆ†æ")
    print(f"  - ä½¿ç”¨ æ–‡æœ¬ç¼–è¾‘å™¨ æ‰“å¼€ _report.txt æŸ¥çœ‹è¯¦ç»†åˆ†æ")
    print(f"  - å…±æŠ“å– {len(df)} æ¡å®Œæ•´æ•°æ®")

def test_connection():
    """æµ‹è¯•è¿æ¥"""
    print("æµ‹è¯•ç½‘ç«™è¿æ¥...")
    try:
        response = requests.get("https://answer.chancefoundation.org.cn/questions", timeout=10)
        if response.status_code == 200:
            print("âœ… ç½‘ç«™å¯æ­£å¸¸è®¿é—®")
            return True
        else:
            print(f"âŒ ç½‘ç«™è®¿é—®å¼‚å¸¸: HTTP {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', 120)
    pd.set_option('display.max_colwidth', 80)
    
    # æµ‹è¯•è¿æ¥
    if test_connection():
        # è¿è¡Œä¸»ç¨‹åº
        main()
    else:
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•")


# In[ ]:




